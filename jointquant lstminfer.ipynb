{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142b7642-cc48-4d61-98ae-4557cba38c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NS\n",
    "#optiver-ver (for ensemble)\n",
    "\n",
    "#optiver-ver-nn-modeling/inference-v1\n",
    "#optiver-ver-rnn-modeling/inference-v1\n",
    "#optiver-ver-lgb-modeling/inference-v1\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import itertools\n",
    "import pickle\n",
    "import joblib\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, GaussianNoise\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.experimental import CosineDecay\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.layers import concatenate,Dropout\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import random\n",
    "from tensorflow.keras.layers import Input, Embedding, Lambda, Reshape, LSTM, Dense, BatchNormalization, Dropout, concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import ZeroPadding1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Lambda, Reshape, LSTM, Dense, BatchNormalization, Dropout, concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import ZeroPadding1D, Activation\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import MaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import Add\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import hmean\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import itertools\n",
    "import pickle\n",
    "import joblib\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, GaussianNoise\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.experimental import CosineDecay\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.layers import concatenate,Dropout\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "import os \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import random\n",
    "#---------------------------------------------------------------------- setup dashboard ------------------------------------------------------------\n",
    "\n",
    "jointquant          = True\n",
    "is_inference        = False\n",
    "load_models         = False\n",
    "run_pipeline        = True\n",
    "train_models        = True\n",
    "is_rnn              = True   \n",
    "online_learning     = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#public-validation\n",
    "dates_train = [\"2018-01-01\",\"2021-06-30\"]\n",
    "dates_test = [\"2021-06-30\",\"2021-06-30\"]\n",
    "\n",
    "#full-inference\n",
    "#dates_train = [0,480]\n",
    "#dates_test = [-1,-1]\n",
    "\n",
    "\n",
    "num_models ={'rnn':1}\n",
    "\n",
    "\n",
    "if jointquant:\n",
    "\n",
    "    train_path = 'A:\\joinquant/train.csv'\n",
    "    models_path = \"infermodel2/\"\n",
    "\n",
    "\n",
    "\n",
    "if dates_train[1]!=\"2023-06-30\":\n",
    "    models_path = \"vals-630-2/\"\n",
    "#---------------------------------------------------------------------- setup dashboard ------------------------------------------------------------\n",
    "\n",
    "simulation_path = \"vals-258/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca844997-696e-429f-829d-32deb60c9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_ep         =45\n",
    "rnn_lr         = 0.001\n",
    "rnn_bs         = 2**12\n",
    "window_size    = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4115763d-35d8-4bca-9bda-933ad1199864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_and_concatenate(year):\n",
    "    # 文件路径\n",
    "    path_1000 = f\"factor/1000/{year}_1000.csv.gz\"\n",
    "    path_1330 = f\"factor/1330/{year}_1330.csv.gz\"\n",
    "    \n",
    "    # 读取数据\n",
    "    df_1000 = pd.read_csv(path_1000, compression='gzip')\n",
    "    df_1330 = pd.read_csv(path_1330, compression='gzip')\n",
    "\n",
    "    # 为每个DataFrame添加时间列\n",
    "    df_1000['time'] = '10:00:00'\n",
    "    df_1330['time'] = '13:30:00'\n",
    "    \n",
    "    # 使用concat而不是merge\n",
    "    concatenated_df = pd.concat([df_1000, df_1330], axis=0)\n",
    "    \n",
    "    # 可以选择排序，如果需要按照日期和代码顺序排列\n",
    "    concatenated_df.sort_values(by=['date', 'code', 'time'], inplace=True)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "# 处理每一年的数据\n",
    "years = [2018,2019,2020,2021]\n",
    "all_data = pd.concat([read_and_concatenate(year) for year in years], axis=0)\n",
    "\n",
    "# all_data 现在包含了所有年份的数据，每个时间点的数据并排放置\n",
    "\n",
    "path_lable = \"label.csv.gz\"\n",
    "lable1 = pd.read_csv(path_lable, compression = \"gzip\") \n",
    "# 合并 all_data 和 label DataFrame\n",
    "train = pd.merge(all_data, lable1, on=['date', 'code',\"time\"], how='inner')\n",
    "\n",
    "# merged_df 现在包含了合并后的数据\n",
    "del lable1 , all_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791be1d0-e296-4662-a4b3-a86e7edd9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('ret_next_5_close_alpha', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "203384d9-3c8f-4998-addc-502aed27da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_price_cols_float32(df):\n",
    "\n",
    "    # all Columns \n",
    "    all_columns = [col for col in df.columns if 'factor' in col]\n",
    "    df[all_columns] = df[all_columns].astype('float32')\n",
    "\n",
    "    return df\n",
    "\n",
    "#train.to_csv('A:\\joinquant/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2090e56-65f7-43fc-a883-88c4f4001601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剔除缺失数据后的唯一天数: 973\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# 按照股票ID和日期进行分组，并计算每组中的唯一时间点数量\n",
    "grouped = train.groupby(['date', 'code'])['time'].nunique()\n",
    "\n",
    "# 找出只有一个时间点的组（即缺失另一个时间点的情况）\n",
    "missing_one_period = grouped[grouped == 1]\n",
    "\n",
    "# 获取缺失一个时间点的组合的索引\n",
    "missing_index = missing_one_period.index\n",
    "\n",
    "# 使用索引剔除这些组\n",
    "filtered_train = train.set_index(['date', 'code'])\n",
    "filtered_train = filtered_train.drop(index=missing_index)\n",
    "\n",
    "# 重置索引\n",
    "filtered_train = filtered_train.reset_index()\n",
    "\n",
    "# 计算剔除缺失数据后的唯一天数\n",
    "unique_days_after_filtering = filtered_train['date'].nunique()\n",
    "print(f\"剔除缺失数据后的唯一天数: {unique_days_after_filtering}\")\n",
    "#filtered_train.to_csv('A:\\joinquant/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25abf4c0-a43b-4e26-8b2e-9d240d63c089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train = filtered_train\n",
    "del filtered_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6d298b-de9f-4d50-a0d9-0483f8aaf5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting column names\n",
      "The 'target' column has 2 NaN values.\n",
      "converting prices columns to float32 values.\n",
      "transforming datetime into float\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#train = pd.read_csv(train_path)\n",
    "\n",
    "print(f\"converting column names\")\n",
    "train = train.rename(columns = {'ret_next_close_alpha':'target','code':'stock_id','date':'date_id'})\n",
    "\n",
    "nan_count = train['target'].isna().sum()\n",
    "print(f\"The 'target' column has {nan_count} NaN values.\")\n",
    "target_median = train['target'].median()\n",
    "train['target'].fillna(target_median, inplace=True)\n",
    "\n",
    "print(f\"converting prices columns to float32 values.\")\n",
    "train = convert_price_cols_float32(train)\n",
    "\n",
    "print(f\"transforming datetime into float\")\n",
    "time_replace_dict = {\n",
    "    '13:30:00': 13.5,\n",
    "    '10:00:00': 10\n",
    "    }\n",
    "train['time'] = train['time'].replace(time_replace_dict)\n",
    "print(\"done\")\n",
    "# ----------------------------- Reading train data -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e995adfa-eccd-4b2b-9da0-3c4cff048756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>factor000</th>\n",
       "      <th>factor001</th>\n",
       "      <th>factor002</th>\n",
       "      <th>factor003</th>\n",
       "      <th>factor004</th>\n",
       "      <th>factor005</th>\n",
       "      <th>factor006</th>\n",
       "      <th>factor007</th>\n",
       "      <th>...</th>\n",
       "      <th>factor492</th>\n",
       "      <th>factor493</th>\n",
       "      <th>factor494</th>\n",
       "      <th>factor495</th>\n",
       "      <th>factor496</th>\n",
       "      <th>factor497</th>\n",
       "      <th>factor498</th>\n",
       "      <th>factor499</th>\n",
       "      <th>time</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>000001.XSHE</td>\n",
       "      <td>-0.8850</td>\n",
       "      <td>1.4940</td>\n",
       "      <td>1.6930</td>\n",
       "      <td>1.7250</td>\n",
       "      <td>-0.3691</td>\n",
       "      <td>-1.2340</td>\n",
       "      <td>-0.48300</td>\n",
       "      <td>1.670</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.164</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>0.4082</td>\n",
       "      <td>-0.3967</td>\n",
       "      <td>1.5960</td>\n",
       "      <td>1.3560</td>\n",
       "      <td>-1.479</td>\n",
       "      <td>-1.6140</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.056046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>000001.XSHE</td>\n",
       "      <td>-0.1461</td>\n",
       "      <td>-0.1818</td>\n",
       "      <td>-0.1921</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>-1.5080</td>\n",
       "      <td>-0.3245</td>\n",
       "      <td>-1.46300</td>\n",
       "      <td>1.485</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.632</td>\n",
       "      <td>0.5650</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>0.8237</td>\n",
       "      <td>1.5450</td>\n",
       "      <td>1.4120</td>\n",
       "      <td>-1.530</td>\n",
       "      <td>0.6973</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-0.044346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>000002.XSHE</td>\n",
       "      <td>1.1860</td>\n",
       "      <td>1.5450</td>\n",
       "      <td>1.3880</td>\n",
       "      <td>1.6310</td>\n",
       "      <td>-1.2130</td>\n",
       "      <td>-1.4330</td>\n",
       "      <td>-1.54900</td>\n",
       "      <td>1.675</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.066</td>\n",
       "      <td>1.0670</td>\n",
       "      <td>-0.4070</td>\n",
       "      <td>-0.6553</td>\n",
       "      <td>1.3470</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>-1.685</td>\n",
       "      <td>-1.5620</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.028517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>000002.XSHE</td>\n",
       "      <td>-1.2690</td>\n",
       "      <td>1.6070</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>1.5840</td>\n",
       "      <td>-1.2070</td>\n",
       "      <td>1.4150</td>\n",
       "      <td>-1.58400</td>\n",
       "      <td>1.609</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.668</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>1.3390</td>\n",
       "      <td>-1.0490</td>\n",
       "      <td>1.5720</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>-1.607</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-0.024816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>000004.XSHE</td>\n",
       "      <td>-1.1990</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>-0.3176</td>\n",
       "      <td>0.7140</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>-0.08673</td>\n",
       "      <td>-1.441</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.259</td>\n",
       "      <td>-0.2119</td>\n",
       "      <td>-0.5186</td>\n",
       "      <td>0.2637</td>\n",
       "      <td>-1.4375</td>\n",
       "      <td>-0.9595</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.6377</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.055232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 504 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_id     stock_id  factor000  factor001  factor002  factor003  \\\n",
       "0  2018-01-02  000001.XSHE    -0.8850     1.4940     1.6930     1.7250   \n",
       "1  2018-01-02  000001.XSHE    -0.1461    -0.1818    -0.1921     0.9470   \n",
       "2  2018-01-02  000002.XSHE     1.1860     1.5450     1.3880     1.6310   \n",
       "3  2018-01-02  000002.XSHE    -1.2690     1.6070     0.8135     1.5840   \n",
       "4  2018-01-02  000004.XSHE    -1.1990     0.2200     0.1958    -0.3176   \n",
       "\n",
       "   factor004  factor005  factor006  factor007  ...  factor492  factor493  \\\n",
       "0    -0.3691    -1.2340   -0.48300      1.670  ...     -1.164     0.0878   \n",
       "1    -1.5080    -0.3245   -1.46300      1.485  ...     -1.632     0.5650   \n",
       "2    -1.2130    -1.4330   -1.54900      1.675  ...     -1.066     1.0670   \n",
       "3    -1.2070     1.4150   -1.58400      1.609  ...     -1.668     0.9995   \n",
       "4     0.7140     0.8413   -0.08673     -1.441  ...     -1.259    -0.2119   \n",
       "\n",
       "   factor494  factor495  factor496  factor497  factor498  factor499  time  \\\n",
       "0     0.4082    -0.3967     1.5960     1.3560     -1.479    -1.6140  10.0   \n",
       "1     0.8916     0.8237     1.5450     1.4120     -1.530     0.6973  13.5   \n",
       "2    -0.4070    -0.6553     1.3470     0.9307     -1.685    -1.5620  10.0   \n",
       "3     1.3390    -1.0490     1.5720     1.0160     -1.607     0.3245  13.5   \n",
       "4    -0.5186     0.2637    -1.4375    -0.9595      0.516     0.6377  10.0   \n",
       "\n",
       "     target  \n",
       "0 -0.056046  \n",
       "1 -0.044346  \n",
       "2 -0.028517  \n",
       "3 -0.024816  \n",
       "4  0.055232  \n",
       "\n",
       "[5 rows x 504 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd2c8eb-2e91-47e3-9019-b6f9b2ca0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title functions\n",
    "\n",
    "def split_by_date(df, dates):\n",
    "\n",
    "    df_start, df_end = dates\n",
    "    df = df[(df['date_id'] >= df_start) & (df['date_id'] <=df_end)].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def lag_function(df, columns_to_lag, numbers_of_days_to_lag):\n",
    "\n",
    "    df_indexed = df.set_index(['stock_id', 'time', 'date_id'])\n",
    "\n",
    "    for column_to_lag in columns_to_lag:\n",
    "        for number_days_to_lag in numbers_of_days_to_lag:\n",
    "            df_indexed[f'lag{number_days_to_lag}_{column_to_lag}'] = df_indexed.groupby(level=['stock_id', 'time'])[column_to_lag].shift(number_days_to_lag)\n",
    "\n",
    "    df_indexed.reset_index(inplace=True)\n",
    "\n",
    "    return df_indexed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_imbalances(df_, columns, prefix = ''):\n",
    "    \"\"\"Computes the differences and imbalances for pairs of columns and stores them in the DataFrame.\"\"\"\n",
    "    df = df_.copy()\n",
    "    for col1, col2 in combinations(columns, 2):\n",
    "\n",
    "        # Sort the columns lexicographically to ensure consistent ordering\n",
    "        col1, col2 = sorted([col1, col2])\n",
    "\n",
    "        # Compute imbalance directly without creating a temporary difference column\n",
    "        total = df[col1] + df[col2]\n",
    "        imbalance_column_name = f'{col1}_{col2}_imb{prefix}'\n",
    "\n",
    "        # Ensure we don't divide by zero\n",
    "        df[imbalance_column_name] = (df[col1] - df[col2]).divide(total, fill_value=np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def save_pickle(data, file_path):\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save the pickle file\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "    print(f\"Data saved to {file_path}\")\n",
    "    #example: save_pickle(all_data, 'k8/all_data.pkl')\n",
    "\n",
    "def load_pickle(file_path):\n",
    "\n",
    "    # Load and return the data from the pickle file\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        return data\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No such file: {file_path}\")\n",
    "\n",
    "    #example: loaded_data = load_pickle('k8/all_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047b79b5-f5ec-4dfd-88a8-641df8257cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_target_lags  = 3\n",
    "target_lags         = list(range(1,num_of_target_lags+1))\n",
    "\n",
    "\n",
    "def feature_pipeline(df):\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "    # df = compute_imbalances(df, columns_sizes,prefix='_sz_')\n",
    "    # df = compute_imbalances(df, columns_prices,prefix = '_pr_')\n",
    "\n",
    "\n",
    "    print(f\"lagging target column for {len(target_lags)} lags.\")\n",
    "    df = lag_function(df, ['target'], target_lags)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89fb1dee-ba41-45b1-95de-dd88556787bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(models, X_test,model = 'nn'):\n",
    "    if model == 'nn':\n",
    "        all_predictions = [model.predict(X_test, batch_size=16384) for model in models]\n",
    "    if model == 'lgb' or model == 'xgb' or model == 'cat':\n",
    "        all_predictions = [model.predict(X_test) for model in models]\n",
    "    prediction = np.mean(all_predictions, axis=0)\n",
    "    return prediction\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "class BestScoresCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_train_loss = float('inf')\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_loss = logs.get('loss', float('inf'))\n",
    "        val_loss = logs.get('val_loss', float('inf'))\n",
    "\n",
    "        if train_loss < self.best_train_loss:\n",
    "            self.best_train_loss = train_loss\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f\"Best training loss: {self.best_train_loss}, Best validation loss: {self.best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2c9516-78d2-4efd-849e-1e5f0fcde351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title RNN second pass\n",
    "\n",
    "def precompute_sequences(stock_data, window_size, rnn_numerical_features, rnn_categorical_features):\n",
    "    # Convert DataFrame columns to NumPy arrays\n",
    "    stock_data_num = stock_data[rnn_numerical_features].values\n",
    "    stock_data_cat = stock_data[rnn_categorical_features].values\n",
    "\n",
    "    # Pre-compute all sequences\n",
    "    all_sequences_num = [stock_data_num[max(0, i - window_size + 1):i + 1] for i in range(len(stock_data))]\n",
    "    all_sequences_cat = [stock_data_cat[max(0, i - window_size + 1):i + 1] for i in range(len(stock_data))]\n",
    "\n",
    "    # Add padding if necessary\n",
    "    padded_sequences_num = [np.pad(seq, ((window_size - len(seq), 0), (0, 0)), 'constant') for seq in all_sequences_num]\n",
    "    padded_sequences_cat = [np.pad(seq, ((window_size - len(seq), 0), (0, 0)), 'constant') for seq in all_sequences_cat]\n",
    "\n",
    "    # Combine numerical and categorical features\n",
    "    combined_sequences = np.array([np.concatenate([num, cat], axis=-1)\n",
    "                                   for num, cat in zip(padded_sequences_num, padded_sequences_cat)])\n",
    "\n",
    "    # Extract targets\n",
    "    targets = stock_data['target'].values\n",
    "\n",
    "    return combined_sequences, targets\n",
    "\n",
    "def get_sequence(precomputed_data, time_step):\n",
    "    combined_sequences, targets = precomputed_data\n",
    "    return combined_sequences[time_step], targets[time_step]\n",
    "\n",
    "\n",
    "\n",
    "def create_batches(data, window_size, rnn_numerical_features, rnn_categorical_features, max_time_steps= 2):\n",
    "\n",
    "    grouped = data.groupby(['stock_id', 'date_id'])\n",
    "    all_batches = []\n",
    "    all_targets = []\n",
    "\n",
    "    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n",
    "        # Precompute sequences for the current group\n",
    "        precomputed_data = precompute_sequences(group, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "\n",
    "        # Initialize containers for group sequences and targets\n",
    "        group_sequences = []\n",
    "        group_targets = []\n",
    "\n",
    "        # Iterate over the time steps and retrieve precomputed sequences\n",
    "        for time_step in range(max_time_steps):\n",
    "            sequence, target = get_sequence(precomputed_data, time_step)\n",
    "            if sequence.size > 0:\n",
    "                group_sequences.append(sequence)\n",
    "                group_targets.append(target)\n",
    "\n",
    "        # Extend the main batches with the group's sequences and targets\n",
    "        all_batches.extend(group_sequences)\n",
    "        all_targets.extend(group_targets)\n",
    "\n",
    "    return all_batches, all_targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_last_sequence(group, window_size, rnn_numerical_features, rnn_categorical_features):\n",
    "    # Convert DataFrame columns to NumPy arrays\n",
    "    stock_data_num      = group[rnn_numerical_features].values\n",
    "    stock_data_cat      = group[rnn_categorical_features].values\n",
    "    stock_data_target   = group['target'].values\n",
    "\n",
    "    # Find the index of the target second\n",
    "    target_index = len(group) - 1\n",
    "\n",
    "    # Extract the sequence for the target index\n",
    "    sequence_num = stock_data_num[max(0, target_index - window_size + 1):target_index + 1]\n",
    "    sequence_cat = stock_data_cat[max(0, target_index - window_size + 1):target_index + 1]\n",
    "\n",
    "    # Add padding if necessary\n",
    "    padded_sequence_num = np.pad(sequence_num, ((window_size - len(sequence_num), 0), (0, 0)), 'constant')\n",
    "    padded_sequence_cat = np.pad(sequence_cat, ((window_size - len(sequence_cat), 0), (0, 0)), 'constant')\n",
    "\n",
    "    # Combine numerical and categorical features\n",
    "    combined_sequence = np.concatenate([padded_sequence_num, padded_sequence_cat], axis=-1)\n",
    "\n",
    "    # Extract target\n",
    "    target = stock_data_target[-1]\n",
    "\n",
    "    return combined_sequence, target\n",
    "\n",
    "\n",
    "def create_last_batches(data, window_size, rnn_numerical_features, rnn_categorical_features):\n",
    "\n",
    "    grouped = data.groupby(['stock_id'])\n",
    "    all_batches = []\n",
    "    all_targets = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        # Compute the sequence for the last data point in the current group\n",
    "        last_sequence, last_target = compute_last_sequence(group, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "\n",
    "        # Check if the sequence is valid (i.e., not empty)\n",
    "        if last_sequence.size > 0:\n",
    "            all_batches.append(last_sequence)\n",
    "            all_targets.append(last_target)\n",
    "\n",
    "    return all_batches, all_targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def second_pass_for_rnn(df, rnn_numerical_features, rnn_categorical_features, window_size, is_inference=False):\n",
    "    # Check if the DataFrame is empty\n",
    "    global rnn_scaler,rnn_medians\n",
    "\n",
    "    if df.empty:\n",
    "        return None, None\n",
    "\n",
    "    # Work on a copy of the DataFrame to avoid changing the original df\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Standard scaling for numerical features\n",
    "    if is_inference:\n",
    "        df_copy.fillna(rnn_medians, inplace=True)\n",
    "        df_copy[rnn_numerical_features] = rnn_scaler.transform(df_copy[rnn_numerical_features])\n",
    "\n",
    "\n",
    "\n",
    "    if is_inference:\n",
    "        df_copy_batches, df_copy_targets = create_last_batches(df_copy, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "    else:\n",
    "        df_copy_batches, df_copy_targets = create_batches(df_copy, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "\n",
    "    df_copy_batches = np.array(df_copy_batches)\n",
    "    df_copy_targets = np.array(df_copy_targets)\n",
    "\n",
    "\n",
    "    return df_copy_batches, df_copy_targets\n",
    "\n",
    "\n",
    "def online_pass_for_rnn(df, rnn_numerical_features, rnn_categorical_features, window_size):\n",
    "    # Check if the DataFrame is empty\n",
    "    global rnn_scaler,rnn_medians\n",
    "\n",
    "    if df.empty:\n",
    "        return None, None\n",
    "\n",
    "    # Work on a copy of the DataFrame to avoid changing the original df\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Standard scaling for numerical features\n",
    "    df_copy.fillna(rnn_medians, inplace=True)\n",
    "    df_copy[rnn_numerical_features] = rnn_scaler.transform(df_copy[rnn_numerical_features])\n",
    "\n",
    "\n",
    "    grouped = df_copy.groupby(['stock_id'])\n",
    "    all_batches = []\n",
    "    all_targets = []\n",
    "\n",
    "    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n",
    "        # Precompute sequences for the current group\n",
    "        precomputed_data = precompute_sequences(group, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "\n",
    "        # Initialize containers for group sequences and targets\n",
    "        group_sequences = []\n",
    "        group_targets = []\n",
    "\n",
    "        # Iterate over the time steps and retrieve precomputed sequences\n",
    "        for time_step in range(2):\n",
    "            sequence, target = get_sequence(precomputed_data, time_step)\n",
    "            if sequence.size > 0:\n",
    "                group_sequences.append(sequence)\n",
    "                group_targets.append(target)\n",
    "\n",
    "        # Extend the main batches with the group's sequences and targets\n",
    "        all_batches.extend(group_sequences)\n",
    "        all_targets.extend(group_targets)\n",
    "\n",
    "    df_batches = np.array(all_batches)\n",
    "    df_targets = np.array(all_targets)\n",
    "\n",
    "\n",
    "    return df_batches, df_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbe2591-54e7-42dd-ac77-26ebde71cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title RNN model\n",
    "from tensorflow.keras.layers import Input, Embedding, Lambda, Reshape, LSTM, Dense, BatchNormalization, Dropout, concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import ZeroPadding1D\n",
    "def apply_conv_layers(input_layer, kernel_sizes, filters= 16, do_ratio=0.5):\n",
    "    conv_outputs = []\n",
    "\n",
    "    for kernel_size in kernel_sizes:\n",
    "        conv_layer = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same')(input_layer)\n",
    "        conv_layer = BatchNormalization()(conv_layer)\n",
    "        conv_layer = Dropout(do_ratio)(conv_layer)\n",
    "\n",
    "        shortcut = conv_layer\n",
    "\n",
    "        conv_layer = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(conv_layer)\n",
    "        conv_layer = BatchNormalization()(conv_layer)\n",
    "        conv_layer = Activation('relu')(conv_layer)\n",
    "\n",
    "        # Add the output of the first Conv1D layer\n",
    "        conv_layer = Add()([conv_layer, shortcut])\n",
    "        conv_outputs.append(conv_layer)\n",
    "\n",
    "\n",
    "    concatenated_conv = Concatenate(axis=-1)(conv_outputs)\n",
    "    flattened_conv_output = Flatten()(concatenated_conv)\n",
    "\n",
    "    return flattened_conv_output\n",
    "\n",
    "def create_rnn_model_with_residual(window_size, numerical_features, initial_learning_rate=0.001):\n",
    "    \n",
    "    categorical_features = 'time'\n",
    "    categorical_uniques  = {'time':2}\n",
    "    embedding_dim        = {'time':2}\n",
    "\n",
    "    input_layer = Input(shape=(window_size, len(numerical_features) + 1), name=\"combined_input\")\n",
    "\n",
    "    # Split the input into numerical and categorical parts\n",
    "    numerical_input = Lambda(lambda x: x[:, :, :-1], name=\"numerical_part\")(input_layer)\n",
    "    categorical_input = Lambda(lambda x: x[:, :, -1:], name=\"categorical_part\")(input_layer)\n",
    "\n",
    "    # Function to create a difference layer for a given lag\n",
    "    def create_difference_layer(lag):\n",
    "        return Lambda(lambda x: x[:, lag:, :] - x[:, :-lag, :], name=f\"difference_layer_lag{lag}\")\n",
    "\n",
    "    # List to store all difference layers\n",
    "    difference_layers = []\n",
    "\n",
    "    # Create difference layers for each lag\n",
    "    for lag in range(1, window_size):\n",
    "        diff_layer = create_difference_layer(lag)(numerical_input)\n",
    "        padding = ZeroPadding1D(padding=(lag, 0))(diff_layer)  # Add padding to the beginning of the sequence\n",
    "        difference_layers.append(padding)\n",
    "\n",
    "    \n",
    "\n",
    "    combined_diff_layer = Concatenate(name=\"combined_difference_layer\")(difference_layers) \n",
    "    \n",
    "    enhanced_numerical_input = Concatenate(name=\"enhanced_numerical_input\")([numerical_input, combined_diff_layer])\n",
    "\n",
    "#     concat_input = Concatenate(name=\"concatenated_input\")([enhanced_numerical_input, categorical_input])\n",
    "\n",
    "    # Embedding for categorical part\n",
    "    vocab_size, embedding_dim = categorical_uniques[categorical_features], embedding_dim[categorical_features]\n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=window_size)(categorical_input)\n",
    "    embedding = Reshape((window_size, -1))(embedding)\n",
    "\n",
    "\n",
    "  \n",
    "    # Concatenate numerical input and embedding\n",
    "    lstm_input = concatenate([enhanced_numerical_input, embedding], axis=-1)\n",
    "\n",
    "    # Initialize a list to hold the outputs of each LSTM layer\n",
    "#     lstm_outputs = []\n",
    "\n",
    "    # First LSTM layer\n",
    "    lstml = LSTM(64, return_sequences=False)(lstm_input)\n",
    "    lstml = BatchNormalization()(lstml)\n",
    "    lstml = Dropout(0.3)(lstml)\n",
    "    \n",
    "    dense_output = lstml\n",
    "    dense_sizes = [512, 256, 128, 64, 32]\n",
    "    do_ratio = 0.3\n",
    "    for size in dense_sizes:\n",
    "        dense_output = Dense(size, activation='swish')(dense_output)\n",
    "        dense_output = BatchNormalization()(dense_output)\n",
    "        dense_output = Dropout(do_ratio)(dense_output)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, name='output_layer')(dense_output)\n",
    "\n",
    "    # Learning rate schedule\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.7,\n",
    "        staircase=True)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mean_absolute_error\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3faa14a0-57b0-4c47-b1c6-5f4634866410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lagging target column for 3 lags.\n",
      "Done...\n",
      "we have 503 numerical and 1 categorical\n",
      "Data saved to vals-630-2/rnn_all_data.pkl\n",
      "Pipline Done!\n",
      "splitting train data\n",
      "splitting test data\n",
      "number of dates in train = 848 , number of dates in test 1\n"
     ]
    }
   ],
   "source": [
    "#@title runnig pipeline\n",
    "\n",
    "excluded_columns = ['date_id','target']\n",
    "\n",
    "if run_pipeline:\n",
    "\n",
    "    train_eng = feature_pipeline(train)\n",
    "    del train\n",
    "    gc.collect()\n",
    "    if is_rnn:\n",
    "        excluded_columns = excluded_columns + ['stock_id']\n",
    "\n",
    "        features = [col for col in train_eng.columns if col not in excluded_columns]\n",
    "        rnn_categorical_features =  ['time']\n",
    "        rnn_numerical_features = [feat for feat in features if feat not in rnn_categorical_features]\n",
    "        print(\"we have {} numerical and {} categorical\".format(len(rnn_numerical_features),len(rnn_categorical_features)))\n",
    "\n",
    "\n",
    "        rnn_scaler = StandardScaler()\n",
    "        rnn_medians = train_eng.median(numeric_only = True)\n",
    "\n",
    "        train_eng.fillna(rnn_medians, inplace=True)\n",
    "        train_eng[rnn_numerical_features] = rnn_scaler.fit_transform(train_eng[rnn_numerical_features])\n",
    "\n",
    "\n",
    "        rnn_all_data = {\n",
    "            \"rnn_scaler\": rnn_scaler,\n",
    "            \"rnn_medians\": rnn_medians,\n",
    "            \"rnn_categorical_features\": rnn_categorical_features,\n",
    "            \"rnn_numerical_features\": rnn_numerical_features\n",
    "        }\n",
    "\n",
    "        save_pickle(rnn_all_data, f'{models_path}rnn_all_data.pkl')\n",
    "        print(\"Pipline Done!\")\n",
    "\n",
    "    print(\"splitting train data\")\n",
    "    train_data       = split_by_date(train_eng, dates_train)\n",
    "    print(\"splitting test data\")\n",
    "    test_data        = split_by_date(train_eng, dates_test)\n",
    "    print(\"number of dates in train = {} , number of dates in test {}\".format (train_data['date_id'].nunique(),test_data['date_id'].nunique()))\n",
    "\n",
    "    cleaning = True\n",
    "    if cleaning:\n",
    "        import gc\n",
    "        #del train\n",
    "        del train_eng\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4a87ff-5a90-410f-872e-73ab7f9c4ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "print('Using TensorFlow version',tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5354908-6a21-4e79-8304-e800def9c1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing groups: 100%|██████████| 2813382/2813382 [54:10<00:00, 865.63it/s]  \n",
      "Processing groups: 100%|██████████| 3711/3711 [00:04<00:00, 896.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches shape:(5626764, 2, 504)\n"
     ]
    }
   ],
   "source": [
    "#@title prepare train models\n",
    "if train_models:\n",
    "\n",
    "    if test_data.empty:\n",
    "        print(\"--------------test data is empty so adjusting the last date for test-----------------\")\n",
    "        test_data = train_data.query(\"date_id == 2021-12-31\").copy()\n",
    "\n",
    "    if is_rnn:\n",
    "\n",
    "        train_batches, train_targets = second_pass_for_rnn(train_data, rnn_numerical_features, rnn_categorical_features, window_size)\n",
    "        del train_data\n",
    "        gc.collect()\n",
    "        test_batches, test_targets  = second_pass_for_rnn(test_data, rnn_numerical_features, rnn_categorical_features, window_size)\n",
    "        del test_data\n",
    "        print(f\"train batches shape:{train_batches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b067787a-a766-45f4-b96b-deff534b92ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20371"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5af3d6fe-80c9-4f0f-8fe9-02516ac7bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rnn model 1 out of 1 with seed 42\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 00:20:44.634126: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 00:20:45.421925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46714 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:81:00.0, compute capability: 8.6\n",
      "2024-01-22 00:20:45.422713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46714 MB memory:  -> device: 1, name: NVIDIA A40, pci bus id: 0000:c1:00.0, compute capability: 8.6\n",
      "2024-01-22 00:20:54.532603: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 22687112448 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 00:21:05.091961: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/1374 [..............................] - ETA: 16s - loss: 1.0675    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 00:21:05.702576: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374/1374 [==============================] - 21s 13ms/step - loss: 0.0561 - val_loss: 0.0241\n",
      "Epoch 2/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0193 - val_loss: 0.0227\n",
      "Epoch 3/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0180 - val_loss: 0.0230\n",
      "Epoch 4/45\n",
      "1374/1374 [==============================] - 17s 13ms/step - loss: 0.0178 - val_loss: 0.0222\n",
      "Epoch 5/45\n",
      "1374/1374 [==============================] - 19s 14ms/step - loss: 0.0172 - val_loss: 0.0217\n",
      "Epoch 6/45\n",
      "1374/1374 [==============================] - 18s 13ms/step - loss: 0.0170 - val_loss: 0.0217\n",
      "Epoch 7/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0169 - val_loss: 0.0216\n",
      "Epoch 8/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0168 - val_loss: 0.0215\n",
      "Epoch 9/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0167 - val_loss: 0.0217\n",
      "Epoch 10/45\n",
      "1374/1374 [==============================] - 17s 13ms/step - loss: 0.0167 - val_loss: 0.0215\n",
      "Epoch 11/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0167 - val_loss: 0.0215\n",
      "Epoch 12/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0166 - val_loss: 0.0214\n",
      "Epoch 13/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0166 - val_loss: 0.0214\n",
      "Epoch 14/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0166 - val_loss: 0.0215\n",
      "Epoch 15/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0166 - val_loss: 0.0214\n",
      "Epoch 16/45\n",
      "1374/1374 [==============================] - 18s 13ms/step - loss: 0.0165 - val_loss: 0.0214\n",
      "Epoch 17/45\n",
      "1374/1374 [==============================] - 17s 13ms/step - loss: 0.0165 - val_loss: 0.0214\n",
      "Epoch 18/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0165 - val_loss: 0.0213\n",
      "Epoch 19/45\n",
      "1374/1374 [==============================] - 17s 13ms/step - loss: 0.0165 - val_loss: 0.0213\n",
      "Epoch 20/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0165 - val_loss: 0.0213\n",
      "Epoch 21/45\n",
      "1374/1374 [==============================] - 17s 13ms/step - loss: 0.0165 - val_loss: 0.0214\n",
      "Epoch 22/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0165 - val_loss: 0.0213\n",
      "Epoch 23/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0164 - val_loss: 0.0213\n",
      "Epoch 24/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0164 - val_loss: 0.0212\n",
      "Epoch 25/45\n",
      "1374/1374 [==============================] - 18s 13ms/step - loss: 0.0164 - val_loss: 0.0213\n",
      "Epoch 26/45\n",
      "1374/1374 [==============================] - 18s 13ms/step - loss: 0.0164 - val_loss: 0.0213\n",
      "Epoch 27/45\n",
      "1374/1374 [==============================] - 17s 13ms/step - loss: 0.0164 - val_loss: 0.0212\n",
      "Epoch 28/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0164 - val_loss: 0.0212\n",
      "Epoch 29/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0164 - val_loss: 0.0212\n",
      "Epoch 30/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0164 - val_loss: 0.0212\n",
      "Epoch 31/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0164 - val_loss: 0.0213\n",
      "Epoch 32/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0164 - val_loss: 0.0212\n",
      "Epoch 33/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0164 - val_loss: 0.0211\n",
      "Epoch 34/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0164 - val_loss: 0.0212\n",
      "Epoch 35/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0164 - val_loss: 0.0212\n",
      "Epoch 36/45\n",
      "1374/1374 [==============================] - 18s 13ms/step - loss: 0.0163 - val_loss: 0.0211\n",
      "Epoch 37/45\n",
      "1374/1374 [==============================] - 18s 13ms/step - loss: 0.0163 - val_loss: 0.0211\n",
      "Epoch 38/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0163 - val_loss: 0.0212\n",
      "Epoch 39/45\n",
      "1374/1374 [==============================] - 17s 13ms/step - loss: 0.0163 - val_loss: 0.0212\n",
      "Epoch 40/45\n",
      "1374/1374 [==============================] - 17s 12ms/step - loss: 0.0163 - val_loss: 0.0211\n",
      "Epoch 41/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0163 - val_loss: 0.0211\n",
      "Epoch 42/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0163 - val_loss: 0.0211\n",
      "Epoch 43/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0163 - val_loss: 0.0211\n",
      "Epoch 44/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0163 - val_loss: 0.0211\n",
      "Epoch 45/45\n",
      "1374/1374 [==============================] - 16s 12ms/step - loss: 0.0163 - val_loss: 0.0210\n",
      "Best training loss: 0.016294896602630615, Best validation loss: 0.020996369421482086\n",
      "---------------------------------------\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Ensemble Mean Absolute Error: 0.0210\n"
     ]
    }
   ],
   "source": [
    "#@title training models\n",
    "# train_data = flatten_outliers(train_data, 'target', lower_quantile=0.01, upper_quantile=0.99)\n",
    "if train_models:\n",
    "\n",
    "        directory = os.path.dirname(models_path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "\n",
    "        if is_rnn:\n",
    "\n",
    "            callbacks = [BestScoresCallback()]  # Always include BestScoresCallback\n",
    "            if dates_train[1] != \"2021-06-30\":\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=70, restore_best_weights=False)\n",
    "                callbacks.append(early_stopping)\n",
    "\n",
    "            if True or tpu_strategy:\n",
    "                # with tpu_strategy.scope():\n",
    "                    rnn_models = []\n",
    "                    for i in range(num_models['rnn']):\n",
    "                        #resetgpu()\n",
    "                        print(f\"Training rnn model {i+1} out of {num_models['rnn']} with seed {42+i}\")\n",
    "                        print(\"---------------------------------------\")\n",
    "                        set_all_seeds(43+i)\n",
    "\n",
    "                        rnn_model = create_rnn_model_with_residual(window_size, rnn_numerical_features, initial_learning_rate=rnn_lr)\n",
    "                        history = rnn_model.fit(train_batches, train_targets, validation_data=(test_batches, test_targets), epochs=rnn_ep, batch_size=rnn_bs, callbacks=callbacks)\n",
    "                        print(\"---------------------------------------\")\n",
    "                        rnn_model.save(f'{models_path}rnn_model_seed_{i+1}.h5')\n",
    "                        rnn_models.append(rnn_model)\n",
    "                     \n",
    "                    predictions =  make_predictions(rnn_models, test_batches,model = 'nn')\n",
    "                    print(f\"Ensemble Mean Absolute Error: {mean_absolute_error(test_targets, predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0498ed-d517-4531-991c-6b15bbce77d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
