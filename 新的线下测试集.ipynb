{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "from warnings import simplefilter\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "is_offline = False\n",
    "is_train = True\n",
    "is_infer = True\n",
    "max_lookback = np.nan\n",
    "split_day = 435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237892, 17)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"A:\\optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed up triplet imbalance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange,jit\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imbtriplet\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n",
    "\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_imb2(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            imbalance_features[j, i] = min_val / max_val\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_imb2_numba(cols, df):\n",
    "    df_values = df[cols].values\n",
    "    comb_indices = [(cols.index(a), cols.index(b), cols.index(c)) for a, b, c in combinations(cols, 3)]\n",
    "    features_array = compute_imb2(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_newimb2\" for a, b, c in combinations(cols, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n",
    "\n",
    "from numba import njit, prange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "@njit\n",
    "def nanmean(arr):\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for val in arr:\n",
    "        if not np.isnan(val):\n",
    "            total += val\n",
    "            count += 1\n",
    "    return total / count if count > 0 else np.nan\n",
    "\n",
    "@njit\n",
    "def nanstd(arr):\n",
    "    mean_val = nanmean(arr)\n",
    "    ssq = 0.0\n",
    "    count = 0\n",
    "    for val in arr:\n",
    "        if not np.isnan(val):\n",
    "            ssq += (val - mean_val) ** 2\n",
    "            count += 1\n",
    "    return np.sqrt(ssq / count) if count > 1 else np.nan\n",
    "\n",
    "@njit\n",
    "def normalize_series_numba(arr):\n",
    "    mean_val = nanmean(arr)\n",
    "    std_val = nanstd(arr)\n",
    "    return (arr - mean_val) / std_val if std_val != 0 else np.full(arr.shape, np.nan)\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_imb3(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    \n",
    "    for i in prange(num_combinations):\n",
    "        a, b = comb_indices[i]\n",
    "        norm_a = normalize_series_numba(df_values[:, a])\n",
    "        norm_b = normalize_series_numba(df_values[:, b])\n",
    "\n",
    "        for j in range(num_rows):\n",
    "            if norm_a[j] + norm_b[j] != 0:  # 避免除以零\n",
    "                imbalance_features[j, i] = (norm_a[j] - norm_b[j]) / (norm_a[j] + norm_b[j])\n",
    "            else:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_imb3_numba(cols, df):\n",
    "    df_values = df[cols].values\n",
    "    comb_indices = [(cols.index(a), cols.index(b)) for a, b in combinations(cols, 2)]\n",
    "    features_array = compute_imb3(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_newimb3\" for a, b in combinations(cols, 2)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n",
    "\n",
    "def calculate_rsi(data, window_size=14):\n",
    "    price_diff = data['wap'].diff()\n",
    "    gain = price_diff.where(price_diff > 0, 0)\n",
    "    loss = -price_diff.where(price_diff < 0, 0)\n",
    "\n",
    "    avg_gain = gain.rolling(window=window_size).mean()\n",
    "    avg_loss = loss.rolling(window=window_size).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def wap_feats(df):\n",
    "    window_size = 7\n",
    "    short_window = 7\n",
    "    long_window = 30\n",
    "\n",
    "    # Group by 'stock_id', 'date_id', and 'time_id' and apply the rolling calculations within each group\n",
    "    #df.loc['rolling_vol'] = df.groupby(['stock_id', 'date_id', 'time_id'])['wap'].pct_change().rolling(window=window_size).std()\n",
    "    df['vol_st'] = df.groupby(['stock_id'])['wap'].pct_change().rolling(window=window_size).std()\n",
    "    df['rolling_vol_di'] = df.groupby(['date_id'])['wap'].pct_change().rolling(window=window_size).std()\n",
    "    df['std_st'] = df.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "    df['wap_pctch'] = df.groupby(['stock_id','date_id'])['wap'].pct_change().values*100\n",
    "    df['short_ema'] = df.groupby(['stock_id'])['wap'].ewm(span=short_window, adjust=False).mean().values\n",
    "    df['long_ema'] = df.groupby(['stock_id'])['wap'].ewm(span=long_window, adjust=False).mean().values\n",
    "    wap_mean = df['wap'].mean()\n",
    "    df['wap_vs_market'] = df['wap'] - df.groupby(['stock_id'])['wap'].transform('mean')\n",
    "    df['macd'] = df['short_ema'] - df['long_ema']\n",
    "    \n",
    "    # Bollinger Bands calculation within each stock, date, and time\n",
    "    df['bollinger_upper'] = df.groupby(['stock_id'])['wap'].rolling(window=long_window).mean().values + 2 * df.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "    df['bollinger_lower'] = df.groupby(['stock_id'])['wap'].rolling(window=long_window).mean().values - 2 * df.groupby(['stock_id'])['wap'].rolling(window=window_size).std().values\n",
    "    # RSI calculation within each stock, date, and time\n",
    "    df['rsi'] = df.groupby(['stock_id']).apply(calculate_rsi).values\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取低重要性特征\n",
    "low_importance_features = pd.read_csv('A:/optiver-trading-at-the-close/lower_than_random_feature_importances.csv')\n",
    "low_importance_feature_names = low_importance_features['Feature'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    alls = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\",\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "        \n",
    "    for c in combinations(sizes, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "  \n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "   \n",
    "    for c in [[\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "        \n",
    "    # 计算 imb2 特征\n",
    " #   for triplet in combinations(prices, 3):\n",
    "    #    imb2_feature = calculate_imb2_numba([triplet[0], triplet[1], triplet[2]], df)\n",
    "   #     for col in imb2_feature.columns:\n",
    "      #      df[col] = imb2_feature[col]\n",
    "    \n",
    "  #  for triplet in combinations(sizes, 3):\n",
    "   #     imb2_feature = calculate_imb2_numba([triplet[0], triplet[1], triplet[2]], df)\n",
    "   #     for col in imb2_feature.columns:\n",
    "     #       df[col] = imb2_feature[col]\n",
    "\n",
    "# 计算 imb3 特征\n",
    "\n",
    "  #  for pair in combinations(prices, 2):\n",
    "     #   imb3_feature = calculate_imb3_numba([pair[0], pair[1]], df)\n",
    "    #    for col in imb3_feature.columns:\n",
    "     #       df[col] = imb3_feature[col]\n",
    "  \n",
    "  #  for pair in combinations(sizes, 2):\n",
    "    #    imb3_feature = calculate_imb3_numba([pair[0], pair[1]], df)\n",
    "    #    for col in imb3_feature.columns:\n",
    "      #      df[col] = imb3_feature[col]\n",
    "        \n",
    "           \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "    cols = ['matched_size','reference_price', \"liquidity_imbalance\",\"all_prices_skew\",'ask_price', 'bid_price', 'ask_size', 'bid_size', 'liquidity_imbalance', 'size_imbalance', \"wap\",'market_urgency']\n",
    "    for col in cols:\n",
    "        for window in [2,3,5,10,15,20,25,30,35]:\n",
    "            col_name = f'{col}_mawithdate_{window}'\n",
    "            df[col_name] = df.groupby(['stock_id', 'date_id'])[col].rolling(window=window).mean(engine='numba').reset_index(level=['stock_id', 'date_id'], drop=True)\n",
    "\n",
    "    cols = ['matched_size','reference_price', \"liquidity_imbalance\",\"all_prices_skew\",'ask_price', 'bid_price', 'ask_size', 'bid_size', 'liquidity_imbalance', 'size_imbalance', \"wap\",'market_urgency']\n",
    "    for col in cols:\n",
    "        for window in [3,5,10,15,20,25,30,35]:\n",
    "            col_name = f'{col}_mstdwithdate_{window}'\n",
    "            df[col_name] = df.groupby(['stock_id', 'date_id'])[col].rolling(window=window).std(engine='numba').reset_index(level=['stock_id', 'date_id'], drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    for col in ['matched_size','reference_price', \"liquidity_imbalance\",\"all_prices_skew\",'ask_price', 'bid_price', 'ask_size', 'bid_size', 'liquidity_imbalance', 'size_imbalance', \"wap\", 'imbalance_buy_sell_flag']:\n",
    "        for window in [1,2,3, 5, 10, 15, 20,25,30,35]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)  \n",
    "            df[f\"{col}_shiftwithdate_{window}\"] = df.groupby(['stock_id','date_id'])[col].shift(window)\n",
    "            df[f\"{col}_retwithdate_{window}\"] = df.groupby(['stock_id','date_id'])[col].pct_change(window)    \n",
    "\n",
    "\n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'market_urgency', 'size_imbalance', \"liquidity_imbalance\", \"all_prices_skew\", \"wap\", 'near_price', 'far_price', 'imbalance_momentum']:\n",
    "        for window in [1,2,3, 5, 10, 15, 20,25,30,35]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "            df[f\"{col}_diffwithdate_{window}\"] = df.groupby(['stock_id','date_id'])[col].diff(window)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# 读取低重要性特征\n",
    "low_importance_features = pd.read_csv('A:/optiver-trading-at-the-close/lower_than_random_feature_importances.csv')\n",
    "low_importance_feature_names = low_importance_features['Feature'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    df = other_features(df)\n",
    "    gc.collect()\n",
    "\n",
    "    # Add a random number feature for feature importance baseline\n",
    "    #df['random_feature'] = np.random.rand(df.shape[0])\n",
    "    # 删除低重要性特征\n",
    "    df.drop(columns=low_importance_feature_names, errors='ignore', inplace=True)\n",
    "    #df = wap_feats(df)\n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]\n",
    "\n",
    "'''def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    df = other_features(df)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\",  \"time_id\"]]\n",
    "    \n",
    "    return df[feature_name]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_offline:\n",
    "    \n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "else:\n",
    "    df_train = df\n",
    "    print(\"Online mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 36.1 GiB for an array with shape (925, 5237892) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     df_valid_feats \u001b[38;5;241m=\u001b[39m reduce_mem_usage(df_valid_feats)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     df_train_feats \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_all_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuild Online Train Feats Finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m df_train_feats \u001b[38;5;241m=\u001b[39m reduce_mem_usage(df_train_feats)\n",
      "Cell \u001b[1;32mIn[6], line 120\u001b[0m, in \u001b[0;36mgenerate_all_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    117\u001b[0m df \u001b[38;5;241m=\u001b[39m df[cols]\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Generate imbalance features\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mimbalance_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m df \u001b[38;5;241m=\u001b[39m other_features(df)\n\u001b[0;32m    122\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[1;32mIn[6], line 95\u001b[0m, in \u001b[0;36mimbalance_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     88\u001b[0m         df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_diff_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstock_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)[col]\u001b[38;5;241m.\u001b[39mdiff(window)\n\u001b[0;32m     89\u001b[0m         df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_diffwithdate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_id\u001b[39m\u001b[38;5;124m'\u001b[39m])[col]\u001b[38;5;241m.\u001b[39mdiff(window)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\frame.py:5238\u001b[0m, in \u001b[0;36mDataFrame.replace\u001b[1;34m(self, to_replace, value, inplace, limit, regex, method)\u001b[0m\n\u001b[0;32m   5228\u001b[0m \u001b[38;5;129m@doc\u001b[39m(NDFrame\u001b[38;5;241m.\u001b[39mreplace, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_shared_doc_kwargs)\n\u001b[0;32m   5229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace\u001b[39m(\n\u001b[0;32m   5230\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5236\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5237\u001b[0m ):\n\u001b[1;32m-> 5238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto_replace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_replace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5241\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\generic.py:6470\u001b[0m, in \u001b[0;36mNDFrame.replace\u001b[1;34m(self, to_replace, value, inplace, limit, regex, method)\u001b[0m\n\u001b[0;32m   6467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bool(regex) \u001b[38;5;129;01mand\u001b[39;00m to_replace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_replace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m if \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 6470\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6473\u001b[0m     \u001b[38;5;66;03m# passing a single value that is scalar like\u001b[39;00m\n\u001b[0;32m   6474\u001b[0m     \u001b[38;5;66;03m# when value is None (GH5319), for compat\u001b[39;00m\n\u001b[0;32m   6475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(to_replace) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(regex):\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\generic.py:5565\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m():\n\u001b[0;32m   5563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mconsolidate()\n\u001b[1;32m-> 5565\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_protect_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\generic.py:5553\u001b[0m, in \u001b[0;36mNDFrame._protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   5551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f()\n\u001b[0;32m   5552\u001b[0m blocks_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[1;32m-> 5553\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks) \u001b[38;5;241m!=\u001b[39m blocks_before:\n\u001b[0;32m   5555\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\generic.py:5563\u001b[0m, in \u001b[0;36mNDFrame._consolidate_inplace.<locals>.f\u001b[1;34m()\u001b[0m\n\u001b[0;32m   5562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m():\n\u001b[1;32m-> 5563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\internals\\managers.py:619\u001b[0m, in \u001b[0;36mBaseBlockManager.consolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    617\u001b[0m bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    618\u001b[0m bm\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 619\u001b[0m \u001b[43mbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bm\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\internals\\managers.py:624\u001b[0m, in \u001b[0;36mBaseBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m--> 624\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1974\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1972\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 1974\u001b[0m     merged_blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   1976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1977\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   1978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_blocks\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2008\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2005\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2007\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2008\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2009\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2011\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 36.1 GiB for an array with shape (925, 5237892) and data type float64"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "    if is_offline:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Train Feats Finished.\")\n",
    "        df_valid_feats = generate_all_features(df_valid)\n",
    "        print(\"Build Valid Feats Finished.\")\n",
    "        df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "    else:\n",
    "        df_train_feats = generate_all_features(df_train)\n",
    "        print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "    df_train_feats = reduce_mem_usage(df_train_feats)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Output the rankings to a CSV file, for example\n",
    "rankings.to_csv(\"A:\\optiver-trading-at-the-close/feature_slope_rankings_2057.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Because this is a time series dataset, we cannot use random KFold to partition the data, which will lead to data leakage (the model will have difficulty converging). Therefore, we strictly ensure that a part of the test set is not used, and divide all *training* data into (train, valid, test) in time series, valid is used as the basis for train iteration, and the optimal parameters are used on train."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    feature_name = list(df_train_feats.columns)\n",
    "    lgb_params = {\n",
    "        \"objective\": \"mae\",\n",
    "        \"n_estimators\": 5500,\n",
    "        \"num_leaves\": 256,\n",
    "        \"subsample\": 0.6,\n",
    "        \"colsample_bytree\": 0.6,\n",
    "        \"learning_rate\": 0.00877,\n",
    "        \"n_jobs\": 4,\n",
    "        'seed': 79,\n",
    "        \"device\": \"gpu\",\n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\": \"gain\",\n",
    "        \"max_depth\": 12,  # Maximum depth of the tree\n",
    "        \"min_child_samples\": 15,  # Minimum number of data points in a leaf\n",
    "        \"reg_alpha\": 0.1,  # L1 regularization term\n",
    "        \"reg_lambda\": 0.3,  # L2 regularization term\n",
    "        \"min_split_gain\": 0.2,  # Minimum loss reduction required for further partitioning\n",
    "        \"min_child_weight\": 0.001,  # Minimum sum of instance weight (hessian) in a leaf\n",
    "        \"bagging_fraction\": 0.9,  # Fraction of data to be used for training each tree\n",
    "        \"bagging_freq\": 5,  # Frequency for bagging\n",
    "        \"feature_fraction\": 0.9,  # Fraction of features to be used for training each tree\n",
    "        \"num_threads\": 4,  # Number of threads for LightGBM to use\n",
    "}\n",
    "    \n",
    "    print(f\"Feature length = {len(feature_name)}\")\n",
    "\n",
    "    offline_split = df_train['date_id']>(split_day - 45)\n",
    "    df_offline_train = df_train_feats[~offline_split]\n",
    "    df_offline_valid = df_train_feats[offline_split]\n",
    "    df_offline_train_target = df_train['target'][~offline_split]\n",
    "    df_offline_valid_target = df_train['target'][offline_split]\n",
    "\n",
    "    print(\"Valid Model Trainning.\")\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_model.fit(\n",
    "        df_offline_train[feature_name],\n",
    "        df_offline_train_target,\n",
    "        eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n",
    "        callbacks=[\n",
    "            lgb.callback.early_stopping(stopping_rounds=100),\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target\n",
    "    gc.collect()\n",
    "\n",
    "    # infer\n",
    "    df_train_target = df_train[\"target\"]\n",
    "    print(\"Infer Model Trainning.\")\n",
    "    infer_params = lgb_params.copy()\n",
    "    infer_params[\"n_estimators\"] = int(1.2 * lgb_model.best_iteration_)\n",
    "    infer_lgb_model = lgb.LGBMRegressor(**infer_params)\n",
    "    infer_lgb_model.fit(df_train_feats[feature_name], df_train_target)\n",
    "\n",
    "    if is_offline:   \n",
    "        # offline predictions\n",
    "        df_valid_target = df_valid[\"target\"]\n",
    "        offline_predictions = infer_lgb_model.predict(df_valid_feats[feature_name])\n",
    "        offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n",
    "        print(f\"Offline Score {np.round(offline_score, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 保存模型为 .pkl 文件\n",
    "model_filename = 'A:\\optiver-trading-at-the-close\\lgb-models-optv2/lgb_model570fe.pkl'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(infer_lgb_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Version  | Feature Description                           | MAE(t/v) offline   | MAE(t/t) online       |  Inferance QPS  | Model              | Lookback |\n",
    "| :-----:  | :-------------------------------------------: | :----------------: | :-------------------: | :-------------: | :----------------: | :------: |\n",
    "| V1       | *Baseline* (41 feats)                         | 5.9712(*5.8578*)   | 5.9161(**5.3693**)    |  0.1887s        | LightGBM           |  /       |\n",
    "| V2       | Add *imbalance* feature (58 feats)            | 5.9589(*5.8465*)   | 5.9055(**5.3700**)    |  0.2351s        | LightGBM           |  /       |\n",
    "| V3       | Add *global* feature (68 feats)      total 147         | 5.9616(*5.8449*)   | 5.9069(**5.3683**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V4       | Add *sliding window* feature (33 feats)   total 184            | 5.94319(*5.8288*)   | 5.88859(**5.3683**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V5       | Add *sliding window* feature (150 feats)   total 336            | 5.94257(*5.8293*)   | 5.88859(**5.3683**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V6       |   total 336  change param for lR and no estimater          | 5.93954(*5.8247*)   | 5.88498(**5.3683**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V7       | change param for lR and no estimater 8500 estimator         | 5.93905(*5.825*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V8       | Add *sliding window* feature (180 feats)   total 408 feature. 5500 estimator         | 5.93896(*5.8245*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V9       | Add *sliding window* feature (120 feats)   total 540 feature.        | 5.93755(*5.8243*)   | 5.88389 (  )  |  /  0.1964s        | LightGBM           |  /       |\n",
    "| V10       | Add imbalance feature (12feats)   total 546 feature.        | 5.93752(*5.8227*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V11       | Add imbalance feature (12feats)   total 570 feature.        | 5.93706(*5.8336*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V12       | Add rolling mean feature  total 616 feature.        | 5.93697(*5.822*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V13       | Add rolling min max feature  total 696 feature.        | 5.93873(*5.824*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V14       | Add rolling std feature  total 648 feature.        | 5.93789(*5.824*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V15       | delete all the feature less important than random number | 5.93739(*5.824*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V16       | delete all the feature less important than random number and add all the big window 658feature| 5.93789(*5.8219*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "| V17       | v7feature total 498feature| 5.93476(*5.8217*)   | /(**/**)    |  0.1964s        | LightGBM           |  /       |\n",
    "> **MAE(t/v)** means train/valid offline scoring, **MAE(t/t)** means train/test online scoring. Inferance QPS means the inference time of a single piece of data (including feature engineering, model prediction). Lookback means the maximum number of lookback seqs for data and models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_infer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m     out \u001b[38;5;241m=\u001b[39m prices\u001b[38;5;241m-\u001b[39mstd_error\u001b[38;5;241m*\u001b[39mstep\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_infer\u001b[49m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptiver2023\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     env \u001b[38;5;241m=\u001b[39m optiver2023\u001b[38;5;241m.\u001b[39mmake_env()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'is_infer' is not defined"
     ]
    }
   ],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step\n",
    "    \n",
    "    return out\n",
    "\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = generate_all_features(cache)[-len(test):]\n",
    "        lgb_prediction = infer_lgb_model.predict(feat)\n",
    "#         lgb_prediction = zero_sum(lgb_prediction, test['bid_size'] + test['ask_size'])\n",
    "        lgb_prediction = lgb_prediction - np.mean(lgb_prediction)\n",
    "        clipped_predictions = np.clip(lgb_prediction, y_min, y_max)\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "           \n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
