{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142b7642-cc48-4d61-98ae-4557cba38c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NS\n",
    "#optiver-ver (for ensemble)\n",
    "\n",
    "#optiver-ver-nn-modeling/inference-v1\n",
    "#optiver-ver-rnn-modeling/inference-v1\n",
    "#optiver-ver-lgb-modeling/inference-v1\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from scipy.stats import hmean\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import itertools\n",
    "import pickle\n",
    "import joblib\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate, GaussianNoise\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.experimental import CosineDecay\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.layers import concatenate,Dropout\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import random\n",
    "from tensorflow.keras.layers import Input, Embedding, Lambda, Reshape, LSTM, Dense, BatchNormalization, Dropout, concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import ZeroPadding1D\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Lambda, Reshape, LSTM, Dense, BatchNormalization, Dropout, concatenate\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import ZeroPadding1D, Activation\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import MaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import Add\n",
    "\n",
    "#---------------------------------------------------------------------- setup dashboard ------------------------------------------------------------\n",
    "\n",
    "jointquant          = True\n",
    "is_inference        = False\n",
    "load_models         = False\n",
    "run_pipeline        = True\n",
    "train_models        = True\n",
    "is_rnn              = True   \n",
    "online_learning     = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#public-validation\n",
    "dates_train = [\"2020-06-01\",\"2021-02-29\"]\n",
    "dates_test = [\"2021-03-01\",\"2021-06-30\"]\n",
    "\n",
    "#full-inference\n",
    "#dates_train = [0,480]\n",
    "#dates_test = [-1,-1]\n",
    "\n",
    "\n",
    "num_models ={'rnn':2}\n",
    "\n",
    "\n",
    "if jointquant:\n",
    "    train_2018_1000_path = \"A:\\joinquant/factor/1000/2018_1000.csv.gz\"\n",
    "    train_2018_1330_path = 'A:\\joinquant/factor/1330/2018_1330.csv.gz'\n",
    "    train_2019_1000_path = \"A:\\joinquant/factor/1000/2019_1000.csv.gz\"\n",
    "    train_2019_1330_path = 'A:\\joinquant/factor/1330/2019_1330.csv.gz'\n",
    "    train_2020_1000_path = \"A:\\joinquant/factor/1000/2020_1000.csv.gz\"\n",
    "    train_2020_1330_path = 'A:\\joinquant/factor/1330/2020_1330.csv.gz'\n",
    "    train_2021_1000_path = \"A:\\joinquant/factor/1000/2021_1000.csv.gz\"\n",
    "    train_2021_1330_path = 'A:\\joinquant/factor/1330/2021_1330.csv.gz'\n",
    "    train_2022_1000_path = \"A:\\joinquant/factor/1000/2022_1000.csv.gz\"\n",
    "    train_2022_1330_path = 'A:\\joinquant/factor/1330/2022_1330.csv.gz'\n",
    "    train_2023_1000_path = \"A:\\joinquant/factor/1000/2023_1000.csv.gz\"\n",
    "    train_2023_1330_path = 'A:\\joinquant/factor/1330/2023_1330.csv.gz'\n",
    "    train_path = 'A:\\joinquant/train.csv'\n",
    "    models_path = \"A:\\joinquant\\model\"\n",
    "else:\n",
    "    models_path = \"A:\\joinquant\\model\"\n",
    "    train_path = \"A:\\joinquant/factor/1000/2018_1000.csv.gz\"\n",
    "    train_path2 = 'A:\\joinquant/factor/1330/2018_1330.csv.gz'\n",
    "    \n",
    "\n",
    "if dates_train[1]!=\"2023-06-30\":\n",
    "    models_path = \"A:\\joinquant/vals-258\"\n",
    "#---------------------------------------------------------------------- setup dashboard ------------------------------------------------------------\n",
    "\n",
    "simulation_path = \"vals-258/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca844997-696e-429f-829d-32deb60c9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_ep         = 1000\n",
    "rnn_lr         = 0.001\n",
    "rnn_bs         = 2**12\n",
    "window_size    = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4115763d-35d8-4bca-9bda-933ad1199864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_concatenate(year):\n",
    "    # 文件路径\n",
    "    path_1000 = f\"A:\\\\joinquant/factor/1000/{year}_1000.csv.gz\"\n",
    "    path_1330 = f\"A:\\\\joinquant/factor/1330/{year}_1330.csv.gz\"\n",
    "    \n",
    "    # 读取数据\n",
    "    df_1000 = pd.read_csv(path_1000, compression='gzip')\n",
    "    df_1330 = pd.read_csv(path_1330, compression='gzip')\n",
    "\n",
    "    # 为每个DataFrame添加时间列\n",
    "    df_1000['time'] = '10:00:00'\n",
    "    df_1330['time'] = '13:30:00'\n",
    "    \n",
    "    # 使用concat而不是merge\n",
    "    concatenated_df = pd.concat([df_1000, df_1330], axis=0)\n",
    "    \n",
    "    # 可以选择排序，如果需要按照日期和代码顺序排列\n",
    "    concatenated_df.sort_values(by=['date', 'code', 'time'], inplace=True)\n",
    "\n",
    "    return concatenated_df\n",
    "\n",
    "# 处理每一年的数据\n",
    "years = [2020,2021]\n",
    "all_data = pd.concat([read_and_concatenate(year) for year in years], axis=0)\n",
    "\n",
    "# all_data 现在包含了所有年份的数据，每个时间点的数据并排放置\n",
    "\n",
    "path_lable = \"A:\\joinquant/label.csv.gz\"\n",
    "lable1 = pd.read_csv(path_lable, compression = \"gzip\") \n",
    "# 合并 all_data 和 label DataFrame\n",
    "train = pd.merge(all_data, lable1, on=['date', 'code',\"time\"], how='inner')\n",
    "\n",
    "# merged_df 现在包含了合并后的数据\n",
    "del lable1 , all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791be1d0-e296-4662-a4b3-a86e7edd9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('ret_next_5_close_alpha', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "203384d9-3c8f-4998-addc-502aed27da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_price_cols_float32(df):\n",
    "\n",
    "    # all Columns \n",
    "    all_columns = [col for col in df.columns if 'factor' in col]\n",
    "    df[all_columns] = df[all_columns].astype('float32')\n",
    "\n",
    "    return df\n",
    "\n",
    "#train.to_csv('A:\\joinquant/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2090e56-65f7-43fc-a883-88c4f4001601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剔除缺失数据后的唯一天数: 486\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# 按照股票ID和日期进行分组，并计算每组中的唯一时间点数量\n",
    "grouped = train.groupby(['date', 'code'])['time'].nunique()\n",
    "\n",
    "# 找出只有一个时间点的组（即缺失另一个时间点的情况）\n",
    "missing_one_period = grouped[grouped == 1]\n",
    "\n",
    "# 获取缺失一个时间点的组合的索引\n",
    "missing_index = missing_one_period.index\n",
    "\n",
    "# 使用索引剔除这些组\n",
    "filtered_train = train.set_index(['date', 'code'])\n",
    "filtered_train = filtered_train.drop(index=missing_index)\n",
    "\n",
    "# 重置索引\n",
    "filtered_train = filtered_train.reset_index()\n",
    "\n",
    "# 计算剔除缺失数据后的唯一天数\n",
    "unique_days_after_filtering = filtered_train['date'].nunique()\n",
    "print(f\"剔除缺失数据后的唯一天数: {unique_days_after_filtering}\")\n",
    "#filtered_train.to_csv('A:\\joinquant/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25abf4c0-a43b-4e26-8b2e-9d240d63c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = filtered_train\n",
    "del filtered_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6d298b-de9f-4d50-a0d9-0483f8aaf5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting column names\n",
      "The 'target' column has 0 NaN values.\n",
      "converting prices columns to float32 values.\n",
      "transforming datetime into float\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#train = pd.read_csv(train_path)\n",
    "\n",
    "print(f\"converting column names\")\n",
    "train = train.rename(columns = {'ret_next_close_alpha':'target','code':'stock_id','date':'date_id'})\n",
    "\n",
    "nan_count = train['target'].isna().sum()\n",
    "print(f\"The 'target' column has {nan_count} NaN values.\")\n",
    "target_median = train['target'].median()\n",
    "train['target'].fillna(target_median, inplace=True)\n",
    "\n",
    "print(f\"converting prices columns to float32 values.\")\n",
    "train = convert_price_cols_float32(train)\n",
    "\n",
    "print(f\"transforming datetime into float\")\n",
    "time_replace_dict = {\n",
    "    '13:30:00': 13.5,\n",
    "    '10:00:00': 10\n",
    "    }\n",
    "train['time'] = train['time'].replace(time_replace_dict)\n",
    "print(\"done\")\n",
    "# ----------------------------- Reading train data -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f463a0a0-8faa-478e-9ca7-bb22f794f28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>factor000</th>\n",
       "      <th>factor001</th>\n",
       "      <th>factor002</th>\n",
       "      <th>factor003</th>\n",
       "      <th>factor004</th>\n",
       "      <th>factor005</th>\n",
       "      <th>factor006</th>\n",
       "      <th>factor007</th>\n",
       "      <th>...</th>\n",
       "      <th>factor492</th>\n",
       "      <th>factor493</th>\n",
       "      <th>factor494</th>\n",
       "      <th>factor495</th>\n",
       "      <th>factor496</th>\n",
       "      <th>factor497</th>\n",
       "      <th>factor498</th>\n",
       "      <th>factor499</th>\n",
       "      <th>time</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>000001.XSHE</td>\n",
       "      <td>1.6170</td>\n",
       "      <td>1.5625</td>\n",
       "      <td>0.59670</td>\n",
       "      <td>0.641000</td>\n",
       "      <td>-0.9060</td>\n",
       "      <td>-0.79740</td>\n",
       "      <td>0.3887</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6690</td>\n",
       "      <td>1.5910</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.05856</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>-1.1820</td>\n",
       "      <td>0.4705</td>\n",
       "      <td>-0.252400</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.017486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>000001.XSHE</td>\n",
       "      <td>1.1840</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>0.05646</td>\n",
       "      <td>1.032000</td>\n",
       "      <td>-0.8530</td>\n",
       "      <td>-0.98970</td>\n",
       "      <td>-1.1440</td>\n",
       "      <td>1.1800</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6580</td>\n",
       "      <td>-0.4045</td>\n",
       "      <td>-0.9010</td>\n",
       "      <td>-0.11444</td>\n",
       "      <td>1.1850</td>\n",
       "      <td>-0.3271</td>\n",
       "      <td>-1.5410</td>\n",
       "      <td>-1.129000</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.014280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>000002.XSHE</td>\n",
       "      <td>-0.1251</td>\n",
       "      <td>1.3260</td>\n",
       "      <td>-1.45300</td>\n",
       "      <td>-0.012115</td>\n",
       "      <td>-0.3140</td>\n",
       "      <td>-0.06256</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>-0.0545</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6990</td>\n",
       "      <td>0.8623</td>\n",
       "      <td>-0.1181</td>\n",
       "      <td>-1.37600</td>\n",
       "      <td>-1.0460</td>\n",
       "      <td>-0.8540</td>\n",
       "      <td>-0.6753</td>\n",
       "      <td>-0.822800</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.050853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>000002.XSHE</td>\n",
       "      <td>0.5874</td>\n",
       "      <td>0.2568</td>\n",
       "      <td>-1.50800</td>\n",
       "      <td>1.003000</td>\n",
       "      <td>0.2305</td>\n",
       "      <td>-1.35450</td>\n",
       "      <td>1.4990</td>\n",
       "      <td>0.1063</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6800</td>\n",
       "      <td>-0.1908</td>\n",
       "      <td>-0.8340</td>\n",
       "      <td>-1.41500</td>\n",
       "      <td>-0.8040</td>\n",
       "      <td>-0.0453</td>\n",
       "      <td>-1.0090</td>\n",
       "      <td>-0.796400</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-0.031760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>000004.XSHE</td>\n",
       "      <td>-0.3320</td>\n",
       "      <td>-1.0400</td>\n",
       "      <td>-1.68600</td>\n",
       "      <td>-0.871000</td>\n",
       "      <td>1.5510</td>\n",
       "      <td>1.65400</td>\n",
       "      <td>-0.2201</td>\n",
       "      <td>-0.7217</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0530</td>\n",
       "      <td>-1.2110</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>1.56900</td>\n",
       "      <td>-0.0424</td>\n",
       "      <td>-1.3630</td>\n",
       "      <td>0.5920</td>\n",
       "      <td>0.878400</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.027291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409703</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>688788.XSHG</td>\n",
       "      <td>0.6055</td>\n",
       "      <td>-1.2730</td>\n",
       "      <td>-1.02050</td>\n",
       "      <td>1.124000</td>\n",
       "      <td>1.1820</td>\n",
       "      <td>1.24800</td>\n",
       "      <td>0.4949</td>\n",
       "      <td>-0.7260</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6970</td>\n",
       "      <td>-1.4590</td>\n",
       "      <td>-1.6720</td>\n",
       "      <td>1.62400</td>\n",
       "      <td>0.2788</td>\n",
       "      <td>-1.2740</td>\n",
       "      <td>0.2078</td>\n",
       "      <td>1.092000</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.003397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409704</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>688981.XSHG</td>\n",
       "      <td>1.6090</td>\n",
       "      <td>0.7280</td>\n",
       "      <td>-1.59000</td>\n",
       "      <td>-1.527000</td>\n",
       "      <td>1.5220</td>\n",
       "      <td>1.45700</td>\n",
       "      <td>1.5400</td>\n",
       "      <td>-1.5310</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3550</td>\n",
       "      <td>-0.1527</td>\n",
       "      <td>-0.3150</td>\n",
       "      <td>0.85840</td>\n",
       "      <td>0.2568</td>\n",
       "      <td>-1.5840</td>\n",
       "      <td>1.2800</td>\n",
       "      <td>-0.583000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.009358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409705</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>688981.XSHG</td>\n",
       "      <td>1.6840</td>\n",
       "      <td>-0.2489</td>\n",
       "      <td>-1.11500</td>\n",
       "      <td>-1.232000</td>\n",
       "      <td>1.6760</td>\n",
       "      <td>0.88130</td>\n",
       "      <td>1.6720</td>\n",
       "      <td>-1.7190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5960</td>\n",
       "      <td>0.1161</td>\n",
       "      <td>1.1270</td>\n",
       "      <td>1.51100</td>\n",
       "      <td>-1.0850</td>\n",
       "      <td>-1.6410</td>\n",
       "      <td>1.5480</td>\n",
       "      <td>1.349000</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-0.008405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409706</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>689009.XSHG</td>\n",
       "      <td>0.3218</td>\n",
       "      <td>-0.2747</td>\n",
       "      <td>-1.02100</td>\n",
       "      <td>-0.064000</td>\n",
       "      <td>1.7230</td>\n",
       "      <td>1.27400</td>\n",
       "      <td>1.6010</td>\n",
       "      <td>-1.7270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.5796</td>\n",
       "      <td>-1.5170</td>\n",
       "      <td>0.21600</td>\n",
       "      <td>-1.2560</td>\n",
       "      <td>-1.6875</td>\n",
       "      <td>1.1540</td>\n",
       "      <td>-1.397000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.001219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409707</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>689009.XSHG</td>\n",
       "      <td>1.3660</td>\n",
       "      <td>1.3780</td>\n",
       "      <td>-1.57900</td>\n",
       "      <td>0.336400</td>\n",
       "      <td>1.7280</td>\n",
       "      <td>0.26340</td>\n",
       "      <td>1.5730</td>\n",
       "      <td>-1.7280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8450</td>\n",
       "      <td>0.8403</td>\n",
       "      <td>-1.7100</td>\n",
       "      <td>1.34000</td>\n",
       "      <td>-1.6500</td>\n",
       "      <td>-1.6680</td>\n",
       "      <td>1.4130</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-0.014037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3409708 rows × 504 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_id     stock_id  factor000  factor001  factor002  factor003  \\\n",
       "0        2020-01-02  000001.XSHE     1.6170     1.5625    0.59670   0.641000   \n",
       "1        2020-01-02  000001.XSHE     1.1840     0.9710    0.05646   1.032000   \n",
       "2        2020-01-02  000002.XSHE    -0.1251     1.3260   -1.45300  -0.012115   \n",
       "3        2020-01-02  000002.XSHE     0.5874     0.2568   -1.50800   1.003000   \n",
       "4        2020-01-02  000004.XSHE    -0.3320    -1.0400   -1.68600  -0.871000   \n",
       "...             ...          ...        ...        ...        ...        ...   \n",
       "3409703  2021-12-31  688788.XSHG     0.6055    -1.2730   -1.02050   1.124000   \n",
       "3409704  2021-12-31  688981.XSHG     1.6090     0.7280   -1.59000  -1.527000   \n",
       "3409705  2021-12-31  688981.XSHG     1.6840    -0.2489   -1.11500  -1.232000   \n",
       "3409706  2021-12-31  689009.XSHG     0.3218    -0.2747   -1.02100  -0.064000   \n",
       "3409707  2021-12-31  689009.XSHG     1.3660     1.3780   -1.57900   0.336400   \n",
       "\n",
       "         factor004  factor005  factor006  factor007  ...  factor492  \\\n",
       "0          -0.9060   -0.79740     0.3887    -0.6290  ...    -1.6690   \n",
       "1          -0.8530   -0.98970    -1.1440     1.1800  ...    -1.6580   \n",
       "2          -0.3140   -0.06256     0.9420    -0.0545  ...    -1.6990   \n",
       "3           0.2305   -1.35450     1.4990     0.1063  ...    -1.6800   \n",
       "4           1.5510    1.65400    -0.2201    -0.7217  ...    -1.0530   \n",
       "...            ...        ...        ...        ...  ...        ...   \n",
       "3409703     1.1820    1.24800     0.4949    -0.7260  ...     1.6970   \n",
       "3409704     1.5220    1.45700     1.5400    -1.5310  ...     1.3550   \n",
       "3409705     1.6760    0.88130     1.6720    -1.7190  ...     0.5960   \n",
       "3409706     1.7230    1.27400     1.6010    -1.7270  ...     0.2397   \n",
       "3409707     1.7280    0.26340     1.5730    -1.7280  ...     0.8450   \n",
       "\n",
       "         factor493  factor494  factor495  factor496  factor497  factor498  \\\n",
       "0           1.5910     0.6390    0.05856     0.9995    -1.1820     0.4705   \n",
       "1          -0.4045    -0.9010   -0.11444     1.1850    -0.3271    -1.5410   \n",
       "2           0.8623    -0.1181   -1.37600    -1.0460    -0.8540    -0.6753   \n",
       "3          -0.1908    -0.8340   -1.41500    -0.8040    -0.0453    -1.0090   \n",
       "4          -1.2110     0.1969    1.56900    -0.0424    -1.3630     0.5920   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "3409703    -1.4590    -1.6720    1.62400     0.2788    -1.2740     0.2078   \n",
       "3409704    -0.1527    -0.3150    0.85840     0.2568    -1.5840     1.2800   \n",
       "3409705     0.1161     1.1270    1.51100    -1.0850    -1.6410     1.5480   \n",
       "3409706     0.5796    -1.5170    0.21600    -1.2560    -1.6875     1.1540   \n",
       "3409707     0.8403    -1.7100    1.34000    -1.6500    -1.6680     1.4130   \n",
       "\n",
       "         factor499  time    target  \n",
       "0        -0.252400  10.0  0.017486  \n",
       "1        -1.129000  13.5  0.014280  \n",
       "2        -0.822800  10.0 -0.050853  \n",
       "3        -0.796400  13.5 -0.031760  \n",
       "4         0.878400  10.0 -0.027291  \n",
       "...            ...   ...       ...  \n",
       "3409703   1.092000  13.5  0.003397  \n",
       "3409704  -0.583000  10.0 -0.009358  \n",
       "3409705   1.349000  13.5 -0.008405  \n",
       "3409706  -1.397000  10.0 -0.001219  \n",
       "3409707   0.002998  13.5 -0.014037  \n",
       "\n",
       "[3409708 rows x 504 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd2c8eb-2e91-47e3-9019-b6f9b2ca0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title functions\n",
    "\n",
    "def split_by_date(df, dates):\n",
    "\n",
    "    df_start, df_end = dates\n",
    "    df = df[(df['date_id'] >= df_start) & (df['date_id'] <=df_end)].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def lag_function(df, columns_to_lag, numbers_of_days_to_lag):\n",
    "\n",
    "    df_indexed = df.set_index(['stock_id', 'time', 'date_id'])\n",
    "\n",
    "    for column_to_lag in columns_to_lag:\n",
    "        for number_days_to_lag in numbers_of_days_to_lag:\n",
    "            df_indexed[f'lag{number_days_to_lag}_{column_to_lag}'] = df_indexed.groupby(level=['stock_id', 'time'])[column_to_lag].shift(number_days_to_lag)\n",
    "\n",
    "    df_indexed.reset_index(inplace=True)\n",
    "\n",
    "    return df_indexed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_imbalances(df_, columns, prefix = ''):\n",
    "    \"\"\"Computes the differences and imbalances for pairs of columns and stores them in the DataFrame.\"\"\"\n",
    "    df = df_.copy()\n",
    "    for col1, col2 in combinations(columns, 2):\n",
    "\n",
    "        # Sort the columns lexicographically to ensure consistent ordering\n",
    "        col1, col2 = sorted([col1, col2])\n",
    "\n",
    "        # Compute imbalance directly without creating a temporary difference column\n",
    "        total = df[col1] + df[col2]\n",
    "        imbalance_column_name = f'{col1}_{col2}_imb{prefix}'\n",
    "\n",
    "        # Ensure we don't divide by zero\n",
    "        df[imbalance_column_name] = (df[col1] - df[col2]).divide(total, fill_value=np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def save_pickle(data, file_path):\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save the pickle file\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "    print(f\"Data saved to {file_path}\")\n",
    "    #example: save_pickle(all_data, 'k8/all_data.pkl')\n",
    "\n",
    "def load_pickle(file_path):\n",
    "\n",
    "    # Load and return the data from the pickle file\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        return data\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No such file: {file_path}\")\n",
    "\n",
    "    #example: loaded_data = load_pickle('k8/all_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44dc1824-eaf2-4606-9675-db33d53c1cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement xelatex (from versions: none)\n",
      "ERROR: No matching distribution found for xelatex\n"
     ]
    }
   ],
   "source": [
    "pip install xelatex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047b79b5-f5ec-4dfd-88a8-641df8257cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_target_lags  = 3\n",
    "target_lags         = list(range(1,num_of_target_lags+1))\n",
    "\n",
    "\n",
    "def feature_pipeline(df):\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "    # df = compute_imbalances(df, columns_sizes,prefix='_sz_')\n",
    "    # df = compute_imbalances(df, columns_prices,prefix = '_pr_')\n",
    "\n",
    "\n",
    "    print(f\"lagging target column for {len(target_lags)} lags.\")\n",
    "    df = lag_function(df, ['target'], target_lags)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Done...\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89fb1dee-ba41-45b1-95de-dd88556787bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(models, X_test,model = 'nn'):\n",
    "    if model == 'nn':\n",
    "        all_predictions = [model.predict(X_test, batch_size=16384) for model in models]\n",
    "    if model == 'lgb' or model == 'xgb' or model == 'cat':\n",
    "        all_predictions = [model.predict(X_test) for model in models]\n",
    "    prediction = np.mean(all_predictions, axis=0)\n",
    "    return prediction\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "class BestScoresCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_train_loss = float('inf')\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        train_loss = logs.get('loss', float('inf'))\n",
    "        val_loss = logs.get('val_loss', float('inf'))\n",
    "\n",
    "        if train_loss < self.best_train_loss:\n",
    "            self.best_train_loss = train_loss\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f\"Best training loss: {self.best_train_loss}, Best validation loss: {self.best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2c9516-78d2-4efd-849e-1e5f0fcde351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title RNN second pass\n",
    "\n",
    "def precompute_sequences(stock_data, window_size, rnn_numerical_features, rnn_categorical_features):\n",
    "    # Convert DataFrame columns to NumPy arrays\n",
    "    stock_data_num = stock_data[rnn_numerical_features].values\n",
    "    stock_data_cat = stock_data[rnn_categorical_features].values\n",
    "\n",
    "    # Pre-compute all sequences\n",
    "    all_sequences_num = [stock_data_num[max(0, i - window_size + 1):i + 1] for i in range(len(stock_data))]\n",
    "    all_sequences_cat = [stock_data_cat[max(0, i - window_size + 1):i + 1] for i in range(len(stock_data))]\n",
    "\n",
    "    # Add padding if necessary\n",
    "    padded_sequences_num = [np.pad(seq, ((window_size - len(seq), 0), (0, 0)), 'constant') for seq in all_sequences_num]\n",
    "    padded_sequences_cat = [np.pad(seq, ((window_size - len(seq), 0), (0, 0)), 'constant') for seq in all_sequences_cat]\n",
    "\n",
    "    # Combine numerical and categorical features\n",
    "    combined_sequences = np.array([np.concatenate([num, cat], axis=-1)\n",
    "                                   for num, cat in zip(padded_sequences_num, padded_sequences_cat)])\n",
    "\n",
    "    # Extract targets\n",
    "    targets = stock_data['target'].values\n",
    "\n",
    "    return combined_sequences, targets\n",
    "\n",
    "def get_sequence(precomputed_data, time_step):\n",
    "    combined_sequences, targets = precomputed_data\n",
    "    return combined_sequences[time_step], targets[time_step]\n",
    "\n",
    "\n",
    "\n",
    "def create_batches(data, window_size, rnn_numerical_features, rnn_categorical_features, max_time_steps= 2):\n",
    "\n",
    "    grouped = data.groupby(['stock_id', 'date_id'])\n",
    "    all_batches = []\n",
    "    all_targets = []\n",
    "\n",
    "    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n",
    "        # Precompute sequences for the current group\n",
    "        precomputed_data = precompute_sequences(group, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "\n",
    "        # Initialize containers for group sequences and targets\n",
    "        group_sequences = []\n",
    "        group_targets = []\n",
    "\n",
    "        # Iterate over the time steps and retrieve precomputed sequences\n",
    "        for time_step in range(max_time_steps):\n",
    "            sequence, target = get_sequence(precomputed_data, time_step)\n",
    "            if sequence.size > 0:\n",
    "                group_sequences.append(sequence)\n",
    "                group_targets.append(target)\n",
    "\n",
    "        # Extend the main batches with the group's sequences and targets\n",
    "        all_batches.extend(group_sequences)\n",
    "        all_targets.extend(group_targets)\n",
    "\n",
    "    return all_batches, all_targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_last_sequence(group, window_size, rnn_numerical_features, rnn_categorical_features):\n",
    "    # Convert DataFrame columns to NumPy arrays\n",
    "    stock_data_num      = group[rnn_numerical_features].values\n",
    "    stock_data_cat      = group[rnn_categorical_features].values\n",
    "    stock_data_target   = group['target'].values\n",
    "\n",
    "    # Find the index of the target second\n",
    "    target_index = len(group) - 1\n",
    "\n",
    "    # Extract the sequence for the target index\n",
    "    sequence_num = stock_data_num[max(0, target_index - window_size + 1):target_index + 1]\n",
    "    sequence_cat = stock_data_cat[max(0, target_index - window_size + 1):target_index + 1]\n",
    "\n",
    "    # Add padding if necessary\n",
    "    padded_sequence_num = np.pad(sequence_num, ((window_size - len(sequence_num), 0), (0, 0)), 'constant')\n",
    "    padded_sequence_cat = np.pad(sequence_cat, ((window_size - len(sequence_cat), 0), (0, 0)), 'constant')\n",
    "\n",
    "    # Combine numerical and categorical features\n",
    "    combined_sequence = np.concatenate([padded_sequence_num, padded_sequence_cat], axis=-1)\n",
    "\n",
    "    # Extract target\n",
    "    target = stock_data_target[-1]\n",
    "\n",
    "    return combined_sequence, target\n",
    "\n",
    "\n",
    "def create_last_batches(data, window_size, rnn_numerical_features, rnn_categorical_features):\n",
    "\n",
    "    grouped = data.groupby(['stock_id'])\n",
    "    all_batches = []\n",
    "    all_targets = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        # Compute the sequence for the last data point in the current group\n",
    "        last_sequence, last_target = compute_last_sequence(group, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "\n",
    "        # Check if the sequence is valid (i.e., not empty)\n",
    "        if last_sequence.size > 0:\n",
    "            all_batches.append(last_sequence)\n",
    "            all_targets.append(last_target)\n",
    "\n",
    "    return all_batches, all_targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def second_pass_for_rnn(df, rnn_numerical_features, rnn_categorical_features, window_size, is_inference=False):\n",
    "    # Check if the DataFrame is empty\n",
    "    global rnn_scaler,rnn_medians\n",
    "\n",
    "    if df.empty:\n",
    "        return None, None\n",
    "\n",
    "    # Work on a copy of the DataFrame to avoid changing the original df\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Standard scaling for numerical features\n",
    "    if is_inference:\n",
    "        df_copy.fillna(rnn_medians, inplace=True)\n",
    "        df_copy[rnn_numerical_features] = rnn_scaler.transform(df_copy[rnn_numerical_features])\n",
    "\n",
    "\n",
    "\n",
    "    if is_inference:\n",
    "        df_copy_batches, df_copy_targets = create_last_batches(df_copy, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "    else:\n",
    "        df_copy_batches, df_copy_targets = create_batches(df_copy, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "\n",
    "    df_copy_batches = np.array(df_copy_batches)\n",
    "    df_copy_targets = np.array(df_copy_targets)\n",
    "\n",
    "\n",
    "    return df_copy_batches, df_copy_targets\n",
    "\n",
    "\n",
    "def online_pass_for_rnn(df, rnn_numerical_features, rnn_categorical_features, window_size):\n",
    "    # Check if the DataFrame is empty\n",
    "    global rnn_scaler,rnn_medians\n",
    "\n",
    "    if df.empty:\n",
    "        return None, None\n",
    "\n",
    "    # Work on a copy of the DataFrame to avoid changing the original df\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Standard scaling for numerical features\n",
    "    df_copy.fillna(rnn_medians, inplace=True)\n",
    "    df_copy[rnn_numerical_features] = rnn_scaler.transform(df_copy[rnn_numerical_features])\n",
    "\n",
    "\n",
    "    grouped = df_copy.groupby(['stock_id'])\n",
    "    all_batches = []\n",
    "    all_targets = []\n",
    "\n",
    "    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n",
    "        # Precompute sequences for the current group\n",
    "        precomputed_data = precompute_sequences(group, window_size, rnn_numerical_features, rnn_categorical_features)\n",
    "\n",
    "        # Initialize containers for group sequences and targets\n",
    "        group_sequences = []\n",
    "        group_targets = []\n",
    "\n",
    "        # Iterate over the time steps and retrieve precomputed sequences\n",
    "        for time_step in range(2):\n",
    "            sequence, target = get_sequence(precomputed_data, time_step)\n",
    "            if sequence.size > 0:\n",
    "                group_sequences.append(sequence)\n",
    "                group_targets.append(target)\n",
    "\n",
    "        # Extend the main batches with the group's sequences and targets\n",
    "        all_batches.extend(group_sequences)\n",
    "        all_targets.extend(group_targets)\n",
    "\n",
    "    df_batches = np.array(all_batches)\n",
    "    df_targets = np.array(all_targets)\n",
    "\n",
    "\n",
    "    return df_batches, df_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbbe2591-54e7-42dd-ac77-26ebde71cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title RNN model\n",
    "\n",
    "def apply_conv_layers(input_layer, kernel_sizes, filters= 16, do_ratio=0.5):\n",
    "    conv_outputs = []\n",
    "\n",
    "    for kernel_size in kernel_sizes:\n",
    "        conv_layer = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same')(input_layer)\n",
    "        conv_layer = BatchNormalization()(conv_layer)\n",
    "        conv_layer = Dropout(do_ratio)(conv_layer)\n",
    "\n",
    "        shortcut = conv_layer\n",
    "\n",
    "        conv_layer = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(conv_layer)\n",
    "        conv_layer = BatchNormalization()(conv_layer)\n",
    "        conv_layer = Activation('relu')(conv_layer)\n",
    "\n",
    "        # Add the output of the first Conv1D layer\n",
    "        conv_layer = Add()([conv_layer, shortcut])\n",
    "        conv_outputs.append(conv_layer)\n",
    "\n",
    "\n",
    "    concatenated_conv = Concatenate(axis=-1)(conv_outputs)\n",
    "    flattened_conv_output = Flatten()(concatenated_conv)\n",
    "\n",
    "    return flattened_conv_output\n",
    "\n",
    "def create_rnn_model_with_residual(window_size, numerical_features, initial_learning_rate=0.001):\n",
    "\n",
    "    categorical_features = 'time'\n",
    "    categorical_uniques  = {'time' : 2}\n",
    "    embedding_dim        = {'time' : 10}\n",
    "\n",
    "    input_layer = Input(shape=(window_size, len(numerical_features) + 1), name=\"combined_input\")\n",
    "\n",
    "    # Split the input into numerical and categorical parts\n",
    "    numerical_input = Lambda(lambda x: x[:, :, :-1], name=\"numerical_part\")(input_layer)\n",
    "    categorical_input = Lambda(lambda x: x[:, :, -1:], name=\"categorical_part\")(input_layer)\n",
    "\n",
    "    first_numerical = Lambda(lambda x: x[:, 0])(numerical_input)\n",
    "\n",
    "\n",
    "    # diffrentiate layers\n",
    "    def create_difference_layer(lag):\n",
    "        return Lambda(lambda x: x[:, lag:, :] - x[:, :-lag, :], name=f\"difference_layer_lag{lag}\")\n",
    "\n",
    "    difference_layers = []\n",
    "    for lag in range(1, window_size):\n",
    "        diff_layer = create_difference_layer(lag)(numerical_input)\n",
    "        padding = ZeroPadding1D(padding=(lag, 0))(diff_layer)  # Add padding to the beginning of the sequence\n",
    "        difference_layers.append(padding)\n",
    "    combined_diff_layer = Concatenate(name=\"combined_difference_layer\")(difference_layers)\n",
    "\n",
    "\n",
    "    # Embedding for categorical part\n",
    "    vocab_size, embedding_dim = categorical_uniques[categorical_features], embedding_dim[categorical_features]\n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=window_size)(categorical_input)\n",
    "    embedding = Reshape((window_size, -1))(embedding)\n",
    "\n",
    "    first_embedding = Lambda(lambda x: x[:, 0])(embedding)\n",
    "\n",
    "    # Concatenate numerical input and embedding\n",
    "    # conv_input = concatenate([enhanced_numerical_input, embedding], axis=-1)\n",
    "\n",
    "    kernel_sizes = [2,3]\n",
    "    do_ratio = 0.4\n",
    "\n",
    "    flattened_conv_output = apply_conv_layers(numerical_input, kernel_sizes, do_ratio=do_ratio)\n",
    "    flattened_conv_output_cat = apply_conv_layers(embedding, kernel_sizes, do_ratio=do_ratio)\n",
    "    flattened_conv_output_diff = apply_conv_layers(combined_diff_layer, kernel_sizes, do_ratio=do_ratio)\n",
    "\n",
    "\n",
    "    dense_output = Concatenate(axis=-1)([flattened_conv_output,flattened_conv_output_cat,flattened_conv_output_diff, Reshape((-1,))(combined_diff_layer),first_numerical,first_embedding])\n",
    "\n",
    "    dense_sizes = [512, 256, 128, 64, 32]\n",
    "    do_ratio = 0.5\n",
    "    for size in dense_sizes:\n",
    "        dense_output = Dense(size, activation='swish')(dense_output)\n",
    "        dense_output = BatchNormalization()(dense_output)\n",
    "        dense_output = Dropout(do_ratio)(dense_output)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(1, name='output_layer')(dense_output)\n",
    "\n",
    "    # Learning rate schedule\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.7,\n",
    "        staircase=True)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"mean_absolute_error\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3faa14a0-57b0-4c47-b1c6-5f4634866410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lagging target column for 3 lags.\n",
      "Done...\n",
      "we have 503 numerical and 1 categorical\n",
      "Data saved to A:\\joinquant/vals-258rnn_all_data.pkl\n",
      "Pipline Done!\n",
      "splitting train data\n",
      "splitting test data\n",
      "number of dates in train = 181 , number of dates in test 83\n"
     ]
    }
   ],
   "source": [
    "#@title runnig pipeline\n",
    "\n",
    "excluded_columns = ['date_id','target']\n",
    "\n",
    "if run_pipeline:\n",
    "\n",
    "    train_eng = feature_pipeline(train)\n",
    "\n",
    "    if is_rnn:\n",
    "        excluded_columns = excluded_columns + ['stock_id']\n",
    "\n",
    "        features = [col for col in train_eng.columns if col not in excluded_columns]\n",
    "        rnn_categorical_features =  ['time']\n",
    "        rnn_numerical_features = [feat for feat in features if feat not in rnn_categorical_features]\n",
    "        print(\"we have {} numerical and {} categorical\".format(len(rnn_numerical_features),len(rnn_categorical_features)))\n",
    "\n",
    "\n",
    "        rnn_scaler = StandardScaler()\n",
    "        rnn_medians = train_eng.median(numeric_only = True)\n",
    "\n",
    "        train_eng.fillna(rnn_medians, inplace=True)\n",
    "        train_eng[rnn_numerical_features] = rnn_scaler.fit_transform(train_eng[rnn_numerical_features])\n",
    "\n",
    "\n",
    "        rnn_all_data = {\n",
    "            \"rnn_scaler\": rnn_scaler,\n",
    "            \"rnn_medians\": rnn_medians,\n",
    "            \"rnn_categorical_features\": rnn_categorical_features,\n",
    "            \"rnn_numerical_features\": rnn_numerical_features\n",
    "        }\n",
    "\n",
    "        save_pickle(rnn_all_data, f'{models_path}rnn_all_data.pkl')\n",
    "        print(\"Pipline Done!\")\n",
    "    cleaning = True\n",
    "    if cleaning:\n",
    "        import gc\n",
    "        del train\n",
    "    print(\"splitting train data\")\n",
    "    train_data       = split_by_date(train_eng, dates_train)\n",
    "    print(\"splitting test data\")\n",
    "    test_data        = split_by_date(train_eng, dates_test)\n",
    "    print(\"number of dates in train = {} , number of dates in test {}\".format (train_data['date_id'].nunique(),test_data['date_id'].nunique()))\n",
    "\n",
    "    cleaning = True\n",
    "    if cleaning:\n",
    "        import gc\n",
    "        #del train\n",
    "        del train_eng\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4a87ff-5a90-410f-872e-73ab7f9c4ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 2.10.1\n",
      "We will restrict TensorFlow to max 8GB GPU RAM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "print('Using TensorFlow version',tf.__version__)\n",
    "\n",
    "# RESTRICT TENSORFLOW TO 8GB OF GPU RAM\n",
    "# SO THAT WE HAVE 8GB RAM FOR RAPIDS\n",
    "LIMIT = 8\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "print('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5354908-6a21-4e79-8304-e800def9c1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing groups: 100%|█████████████████████████████████████████████████████| 616466/616466 [07:57<00:00, 1292.19it/s]\n",
      "Processing groups: 100%|█████████████████████████████████████████████████████| 294631/294631 [03:39<00:00, 1342.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches shape:(1232932, 2, 504)\n"
     ]
    }
   ],
   "source": [
    "#@title prepare train models\n",
    "if train_models:\n",
    "\n",
    "    if test_data.empty:\n",
    "        print(\"--------------test data is empty so adjusting the last date for test-----------------\")\n",
    "        test_data = train_data.query(\"date_id == 2021-12-31\").copy()\n",
    "\n",
    "    if is_rnn:\n",
    "\n",
    "        train_batches, train_targets = second_pass_for_rnn(train_data, rnn_numerical_features, rnn_categorical_features, window_size)\n",
    "        del train_data\n",
    "        test_batches, test_targets  = second_pass_for_rnn(test_data, rnn_numerical_features, rnn_categorical_features, window_size)\n",
    "        del test_data\n",
    "        print(f\"train batches shape:{train_batches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5af3d6fe-80c9-4f0f-8fe9-02516ac7bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rnn model 1 out of 2 with seed 42\n",
      "---------------------------------------\n",
      "Epoch 1/1000\n",
      "302/302 [==============================] - 20s 37ms/step - loss: 0.2039 - val_loss: 0.0355\n",
      "Epoch 2/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0403 - val_loss: 0.0208\n",
      "Epoch 3/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0317 - val_loss: 0.0210\n",
      "Epoch 4/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0280 - val_loss: 0.0195\n",
      "Epoch 5/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0237 - val_loss: 0.0196\n",
      "Epoch 6/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0208 - val_loss: 0.0192\n",
      "Epoch 7/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0196\n",
      "Epoch 8/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0203 - val_loss: 0.0195\n",
      "Epoch 9/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0202 - val_loss: 0.0193\n",
      "Epoch 10/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0193\n",
      "Epoch 11/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0193\n",
      "Epoch 12/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0193\n",
      "Epoch 13/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0193\n",
      "Epoch 14/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0194\n",
      "Epoch 15/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0202 - val_loss: 0.0192\n",
      "Epoch 16/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0200 - val_loss: 0.0194\n",
      "Epoch 17/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0193\n",
      "Epoch 18/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0192\n",
      "Epoch 19/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0200 - val_loss: 0.0194\n",
      "Epoch 20/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0201 - val_loss: 0.0194\n",
      "Epoch 21/1000\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.0201 - val_loss: 0.0191\n",
      "Epoch 22/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0200 - val_loss: 0.0193\n",
      "Epoch 23/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0192\n",
      "Epoch 24/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0191\n",
      "Epoch 25/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0200 - val_loss: 0.0192\n",
      "Epoch 26/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0192\n",
      "Epoch 27/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0201 - val_loss: 0.0191\n",
      "Epoch 28/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0200 - val_loss: 0.0192\n",
      "Epoch 29/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0200 - val_loss: 0.0192\n",
      "Epoch 30/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0200 - val_loss: 0.0192\n",
      "Epoch 31/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0200 - val_loss: 0.0192\n",
      "Epoch 32/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0200 - val_loss: 0.0192\n",
      "Epoch 33/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0200 - val_loss: 0.0191\n",
      "Epoch 34/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 35/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0199 - val_loss: 0.0192\n",
      "Epoch 36/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 37/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0199 - val_loss: 0.0192\n",
      "Epoch 38/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0199 - val_loss: 0.0193\n",
      "Epoch 39/1000\n",
      "302/302 [==============================] - 10s 32ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 40/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 41/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 42/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 43/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 44/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 45/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0199 - val_loss: 0.0191\n",
      "Epoch 46/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0198 - val_loss: 0.0191\n",
      "Epoch 47/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0197 - val_loss: 0.0189\n",
      "Epoch 48/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0195 - val_loss: 0.0186\n",
      "Epoch 49/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0193 - val_loss: 0.0185\n",
      "Epoch 50/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0192 - val_loss: 0.0183\n",
      "Epoch 51/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0190 - val_loss: 0.0182\n",
      "Epoch 52/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0189 - val_loss: 0.0182\n",
      "Epoch 53/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0189 - val_loss: 0.0181\n",
      "Epoch 54/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0188 - val_loss: 0.0180\n",
      "Epoch 55/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0188 - val_loss: 0.0180\n",
      "Epoch 56/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0188 - val_loss: 0.0180\n",
      "Epoch 57/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0187 - val_loss: 0.0180\n",
      "Epoch 58/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0187 - val_loss: 0.0180\n",
      "Epoch 59/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0187 - val_loss: 0.0180\n",
      "Epoch 60/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0187 - val_loss: 0.0180\n",
      "Epoch 61/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0187 - val_loss: 0.0180\n",
      "Epoch 62/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0186 - val_loss: 0.0180\n",
      "Epoch 63/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0186 - val_loss: 0.0179\n",
      "Epoch 64/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0186 - val_loss: 0.0180\n",
      "Epoch 65/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0186 - val_loss: 0.0179\n",
      "Epoch 66/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0186 - val_loss: 0.0179\n",
      "Epoch 67/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0186 - val_loss: 0.0180\n",
      "Epoch 68/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0186 - val_loss: 0.0179\n",
      "Epoch 69/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0185 - val_loss: 0.0180\n",
      "Epoch 70/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 71/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 72/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 73/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0185 - val_loss: 0.0180\n",
      "Epoch 74/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 75/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 76/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 77/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 78/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 79/1000\n",
      "302/302 [==============================] - 9s 30ms/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 80/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Epoch 81/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Epoch 82/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Epoch 83/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Epoch 84/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Epoch 85/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Epoch 86/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Epoch 87/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Epoch 88/1000\n",
      "302/302 [==============================] - 9s 31ms/step - loss: 0.0184 - val_loss: 0.0179\n",
      "Best training loss: 0.01835629902780056, Best validation loss: 0.017911072820425034\n",
      "---------------------------------------\n",
      "Training rnn model 2 out of 2 with seed 43\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m set_all_seeds(\u001b[38;5;241m42\u001b[39m\u001b[38;5;241m+\u001b[39mi)\n\u001b[0;32m     26\u001b[0m rnn_model \u001b[38;5;241m=\u001b[39m create_rnn_model_with_residual(window_size, rnn_numerical_features, initial_learning_rate\u001b[38;5;241m=\u001b[39mrnn_lr)\n\u001b[1;32m---> 27\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_targets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrnn_ep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrnn_bs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m rnn_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mrnn_model_seed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py310_tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\py310_tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "#@title training models\n",
    "# train_data = flatten_outliers(train_data, 'target', lower_quantile=0.01, upper_quantile=0.99)\n",
    "if train_models:\n",
    "\n",
    "        directory = os.path.dirname(models_path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "\n",
    "        if is_rnn:\n",
    "\n",
    "            callbacks = [BestScoresCallback()]  # Always include BestScoresCallback\n",
    "            if dates_train[1] != \"2021-12-31\":\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
    "                callbacks.append(early_stopping)\n",
    "\n",
    "            if True or tpu_strategy:\n",
    "                # with tpu_strategy.scope():\n",
    "                    rnn_models = []\n",
    "                    for i in range(num_models['rnn']):\n",
    "                        #resetgpu()\n",
    "                        print(f\"Training rnn model {i+1} out of {num_models['rnn']} with seed {42+i}\")\n",
    "                        print(\"---------------------------------------\")\n",
    "                        set_all_seeds(42+i)\n",
    "\n",
    "                        rnn_model = create_rnn_model_with_residual(window_size, rnn_numerical_features, initial_learning_rate=rnn_lr)\n",
    "                        history = rnn_model.fit(train_batches, train_targets, validation_data=(test_batches, test_targets), epochs=rnn_ep, batch_size=rnn_bs, callbacks=callbacks)\n",
    "                        print(\"---------------------------------------\")\n",
    "                        rnn_model.save(f'{models_path}rnn_model_seed_{i+1}.h5')\n",
    "                        rnn_models.append(rnn_model)\n",
    "                     \n",
    "                    predictions =  make_predictions(rnn_models, test_batches,model = 'nn')\n",
    "                    print(f\"Ensemble Mean Absolute Error: {mean_absolute_error(test_targets, predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab63c87-3c7a-4934-86e0-85d9092d9e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 2.10.1\n",
      "We will restrict TensorFlow to max 8GB GPU RAM\n",
      "Training rnn model 2 out of 2 with seed 43\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "print('Using TensorFlow version',tf.__version__)\n",
    "\n",
    "# RESTRICT TENSORFLOW TO 8GB OF GPU RAM\n",
    "# SO THAT WE HAVE 8GB RAM FOR RAPIDS\n",
    "LIMIT = 8\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "print('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\n",
    "\n",
    "print(f\"Training rnn model {i+1} out of {num_models['rnn']} with seed {42+i}\")\n",
    "print(\"---------------------------------------\")\n",
    "set_all_seeds(42+i)\n",
    "\n",
    "rnn_model = create_rnn_model_with_residual(window_size, rnn_numerical_features, initial_learning_rate=rnn_lr)\n",
    "history = rnn_model.fit(train_batches, train_targets, validation_data=(test_batches, test_targets), epochs=rnn_ep, batch_size=rnn_bs, callbacks=callbacks)\n",
    "print(\"---------------------------------------\")\n",
    "rnn_model.save(f'{models_path}rnn_model_seed_{i+1}.h5')\n",
    "rnn_models.append(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536496f-effe-440e-a752-403384df92cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_tf",
   "language": "python",
   "name": "py310_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
